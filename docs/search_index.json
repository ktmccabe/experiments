[["index.html", "Experimental Methods in Political Science Section 1 Course Notes", " Experimental Methods in Political Science Instructor: Katie McCabe Section 1 Course Notes This document will include important links and course notes for Experimental Methods in Political Science. This site will be updated throughout the semester with new content. The Canvas modules will provide links to the relevant sections to review for a given week of the course. The primary text for the course is Field Experiments: Design, Analysis, and Interpretation by Alan Gerber and Don Green. We will refer to this as FEDAI in the notes. This is a new and living document. If you spot errors or have questions or suggestions, please email me at k.mccabe@rutgers.edu or send a Slack message. "],["rsetup.html", "1.1 Setup in R", " 1.1 Setup in R Goal By the end of the first week of the course, you will want to have R and RStudio installed on your computer (both free) and feel comfortable using R as a calculator. R is an application that processes the R programming language in a statistical computing environment. RStudio is also an application, which serves as a user interface that makes working in R easier. We will primarily open and use RStudio to work with R. In other classes, you may come across Stata, SPSS, Excel, or SAS, which are programs that also conduct data analysis. R has the advantage of being free and open-source. Even after you leave the university setting, you will be able to use R/RStudio for free. As an open-source program, it is very flexible, and a community of active R/RStudio users is constantly adding to and improving the program. R and RStudio Installation This content follows and reinforces QSS 1.3 by Kosuke Imai. Additional resources are also linked below. This video from Professor Christopher Bail explains why many social scientists use R and describes the R and RStudio installation process. This involves Going to cran, select the link that matches your operating system, and then follow the installation instructions, and Visiting RStudio and follow the download and installation instructions. R is the statistical software and programming language used for analysis. RStudio provides a convenient user interface for running R code. "],["first-time-working-in-r-and-rstudio.html", "1.2 First Time Working in R and RStudio", " 1.2 First Time Working in R and RStudio This next section provides a few notes on using R and RStudio now that you have installed it. In this section, we cover the following materials: Using R as a calculator and assigning objects using &lt;- Setting your working directory and the setwd() function. Creating and saving an R script 1.2.1 Open RStudio RStudio is an open-source and free program that greatly facilitates the use of R, especially for users new to programming. Once you have downloaded and installed R and RStudio, to work in R, all you need to do now is open RStudio (it will open R). It should look like this, though your version numbers will be different: Note: The first time you open RStudio, you likely only have the three windows above. We will want to create a fourth window by opening an R script to create the fourth window. To do this, in RStudio, click on File -&gt; New -&gt; R script in your computer’s toolbar. This will open a blank document for text editing in the upper left of the RStudio window. We will return to this window in a moment. You can alternatively click on the green + sign indicator in the top-left corner of the RStudio window, which should give you the option to create a new R script document. Now you should have something that looks like this, similar to Figure 1.1. in QSS: The upper-left window has our script document that will contain code. The lower-left window is the console. This will show the output of the code we run. We will also be able to type directly in the console. The upper-right window shows the environment (and other tabs, such as the history of commands). When we load and store data in RStudio, we will see a summary of that in the environment. The lower-right window will enable us to view plots and search help files, among other things. 1.2.2 Using R as a Calculator The bottom left window in your RStudio is the Console. You can type in this window to use R as a calculator or to try out commands. It will show the raw output of any commands you type. For example, we can try to use R as a calculator. Type the following in the Console (the bottom left window) and hit “enter” or “return” on your keyboard: 5 + 3 ## [1] 8 5 - 3 ## [1] 2 5^2 ## [1] 25 5 * 3 ## [1] 15 5/3 ## [1] 1.666667 (5 + 3) * 2 ## [1] 16 In the other RStudio windows, the upper right will show a history of commands that you have sent from the text editor to the R console, along with other items. The lower right will show graphs, help documents and other features. These will be useful later in the course. 1.2.3 Working in an R Script Earlier, I asked you to open an R script in the upper left window by doing File, then New File, then R Script. Let’s go back to working in that window. Set your working directory setwd() (Almost) every time you work in RStudio, the first thing you will do is set your working directory. This is a designated folder in your computer where you will save your R scripts and datasets. There are many ways to do this. An easy way is to go to Session -&gt; Set Working Directory -&gt; Choose Directory. I suggest choosing a folder in your computer that you can easily find and that you will routinely use for this class. Go ahead and create/select it. Note: when you selected your directory, code came out in the bottom left Console window. This is the setwd() command which can also be used directly to set your working directory in the future. If you aren’t sure where your directory has been set, you can also type getwd() in your Console. Try it now ## Example of where my directory was getwd() If I want to change the working directory, I can go to the top toolbar of my computer and use Session -&gt; Set Working Directory -&gt; Choose Directory or just type my file pathway using the setwd() below: ## Example of setting the working directory using setwd(). ## Your computer will have your own file path. setwd(&quot;/Users/ktmccabe/Dropbox/Rutgers Teaching/&quot;) Saving the R Script Let’s now save our R script to our working directory and give it an informative name. To do so, go to File, then Save As, make sure you are in the same folder on your computer as the folder you chose for your working directory. Give the file an informative name, such as: “McCabeWeek1.R”. Note: all of your R scripts will have the .R extension. 1.2.4 Preparing your R script Now that we have saved our R script, let’s work inside of it. Remember, we are in the top-left RStudio window now. Just like the beginning of a paper, you will want to title your R script. In R, any line that you start with a # will not be treated as a programming command. You can use this to your advantage to write titles/comments. Below is a screenshot example of a template R script. You can specify your working directory at the top, too. Add your own filepath inside setwd() Then you can start answering problems in the rest of the script. Think of the R script as where you write the final draft of your paper. In the Console (the bottom-left window), you can mess around and try different things, like you might when you are taking notes or outlining an essay. Then, write the final programming steps that lead you to your answer in the R script. For example, if I wanted to add 5 + 3, I might try different ways of typing it in the Console, and then when I found out 5 + 3 is the right approach, I would type that into my script. 1.2.5 Executing Commands in your R script The last thing we will note in this initial handout is how to execute commands in your R script. To run / execute a command in your R script (the upper left window), you can Highlight the code you want to run, and then hold down “command + return” on a Mac or “control + enter” on Windows Place your cursor at the end of the line of code (far right), and hit “command + return” on a Mac or “control + return” on Windows, or Do 1 or 2, but instead of using the keyboard to execute the commands, click “Run” in the top right corner of the upper-left window. Try it: Type 5 + 3 in the R script. Then, try to execute 5 + 3. It should look something like this: After you executed the code, you should see it pop out in your Console: 5 + 3 ## [1] 8 Note: The symbol # also allows for annotation behind commands or on a separate line. Everything that follows # will be ignored by R. You can annotate your own code so that you and others can understand what each part of the code is designed to do. ## Example sum53 &lt;- 5 + 3 # example of assigning an addition calculation 1.2.6 Objects Sometimes we will want to store our calculations as “objects” in R. We use &lt;- to assign objects by placing it to the left of what we want to store. For example, let’s store the calculation 5 + 3 as an object named sum53: sum53 &lt;- 5 + 3 After we execute this code, sum53 now stores the calculation. This means, that if we execute a line of code that just hassum53`, it will output 8. Try it: sum53 ## [1] 8 Now we no longer have to type 5 + 3, we can just type sum53. For example, let’s say we wanted to subtract 2 from this calculation. We could do: sum53 - 2 ## [1] 6 Let’s say we wanted to divide two stored calculations: ten &lt;- 5 + 5 two &lt;- 1 + 1 ten / two ## [1] 5 The information stored does not have to be numeric. For example, it can be a word, or what we would call a character string, in which case you need to use quotation marks. mccabe &lt;- &quot;professor for this course&quot; mccabe ## [1] &quot;professor for this course&quot; Note: Object names cannot begin with numbers and no spacing is allowed. Avoid using special characters such as % and $, which have specific meanings in R. Finally, use concise and intuitive object names. GOOD CODE: practice.calc &lt;- 5 + 3 BAD CODE: meaningless.and.unnecessarily.long.name &lt;- 5 + 3 While these are simple examples, we will use objects all the time for more complicated things to store (e.g., like full datasets!) throughout the course. We can also store an array or “vector” of information using c() somenumbers &lt;- c(3, 6, 8, 9) somenumbers ## [1] 3 6 8 9 Importance of Clean Code Ideally, when you are done with your R script, you should be able to highlight the entire script and execute it without generating any error messages. This means your code is clean. Code with typos in it may generate a red error message in the Console upon execution. This can happen when there are typos or commands are misused. For example, R is case sensitive. Let’s say we assigned our object like before: sum53 &lt;- 5 + 3 However, when we went to execute sum53, we accidentally typed Sum53: Sum53 ## Error in eval(expr, envir, enclos): object &#39;Sum53&#39; not found Only certain types of objects can be used in mathematical calculations. Let’s say we tried to divide mccabe by 2: mccabe / 2 ## Error in mccabe/2: non-numeric argument to binary operator A big part of learning to use R will be learning how to troubleshoot and detect typos in your code that generate error messages. 1.2.7 Practice with R Scripts Below is an exercise that will demonstrate you are able to use R as a calculator and create R scripts. Create an R script saved as ``LastnameSetup1.R” (use your last name). Within the R script, follow the example from this handout and title the script. Set your working directory, and include the file pathway (within setwd()) at the top of your .R script. Do the calculation 8 + 3 - 2 in R. Store it as an object with an informative name. Report the answer. Do the calculation 5 x 3 in R. Store it as an object with an informative name. Report the answer. Add these two calculations together. Note: do this by adding together the objects you created, not the underlying raw calculations. Report the answer. 1.2.8 Loading data into R Often the variables we care about are stored inside of rectangular datasets, like the dataset on turnout below from QSS Chapter 1. These have a number of rows nrow() and columns ncol() Each row is an “observation,” representing the information collected from an individual or entity Each column is a variable, representing a changing characteristic across multiple observations When we import a dataset into R, we have a few options. This highlights key elements of QSS section 1.3.5, which provides an overview of loading data into R with an example using UN population data. Option 1: Download dataset to your computer Move the dataset to your working directory Identify the file type (e.g., csv, dta, RData, txt) Pick the appropriate R function to match the type (e.g., read.csv(), read.dta(), load(), read.table()) Assign the dataset to an object. This object will now be class() of data.frame ## The turnout dataset is available in the week 2 Canvas module turnout &lt;- read.csv(&quot;turnout.csv&quot;) With a recent update to R, many people now add an argument to their code when loading in .csv files, which makes it easier to work with categorical variables. turnout &lt;- read.csv(&quot;turnout.csv&quot;, stringsAsFactors = T) There are also packages that exist, which simplify loading different types of data. Example: install.packages(&quot;rio&quot;, dependencies = T) Once you’ve installed it, you can load the package with library(). While you only need to install the package once, you need to use library() in each script where you use import. library(rio) turnout &lt;- import(&quot;turnout.csv&quot;) Option 2: Read file from a url provided Need an active internet connection for this to work Need friendly file type URL generally must be public Include the url inside the function used to read the data turnout &lt;- read.csv(&quot;https://raw.githubusercontent.com/ktmccabe/teachingdata/main/turnout.csv&quot;) class(turnout) ## [1] &quot;data.frame&quot; You can also open up a window to view the data: View(turnout) And you can view the first few rows with head() head(turnout) ## year VEP VAP total ANES felons noncit overseas osvoters ## 1 1980 159635 164445 86515 71 802 5756 1803 NA ## 2 1982 160467 166028 67616 60 960 6641 1982 NA ## 3 1984 167702 173995 92653 74 1165 7482 2361 NA ## 4 1986 170396 177922 64991 53 1367 8362 2216 NA ## 5 1988 173579 181955 91595 70 1594 9280 2257 NA ## 6 1990 176629 186159 67859 47 1901 10239 2659 NA Note that the columns are the variables, and each row contains the values corresponding to the variables for different units in the data. Here, each row is an election year. In many datasets we will work with, each row will be an experimental subject. We can access specific columns in the data using the $ attached to the dataframe name. For example, the code below displays all of the values from the year variable in the turnout dataframe: turnout$year ## [1] 1980 1982 1984 1986 1988 1990 1992 1994 1996 1998 2000 2002 2004 2008 This will be useful when we want to summarize particular variables, such as by finding the median. median(turnout$year) ## [1] 1993 "],["r-markdown.html", "1.3 R Markdown", " 1.3 R Markdown An R Markdown document, which you can create in RStudio, allows you to weave together regular text, R code, and the output of R code in the same document. This can be very convenient when conducting data analysis because it allows you more space to explain what you are doing in each step. It can also be an effective platform for writing a report on a data analysis, similar to what you do when you write up a problem set. It can also be useful for organizing replication files to post after you publish a paper. R Markdown documents can be “compiled” into html, pdf, or docx documents. Below is an example of what a compiled html file looks like. Note that the image has both written text and a gray chunk, within which there is some R code, as well as the output of the R code (e.g., the number 8 and the image of the histogram plot) We say this is a “compiled” RMarkdown document because it differs from the raw version of the file, which is a .Rmd file format. Below is an example of what the raw .Rmd version looks like, compared to the compiled html version. 1.3.1 How to get setup in RMarkdown Just like with a regular R script, to work in RMarkdown, you will open up RStudio. The first time you will be working in RMarkdown, you will want to install two packages: rmarkdown and knitr. You can do this in the Console window in RStudio. Type the following into the Console window and hit enter/return. install.packages(&quot;rmarkdown&quot;) install.packages(&quot;knitr&quot;) Once you have those installed, now, each time you want to create an RMarkdown document, you will open up a .Rmd R Markdown file and get to work. Go to File -&gt; New File -&gt; R Markdown in RStudio Alternatively, you can click the green + symbol at the top left of your RStudio window This should open up a window with several options, similar to the image below Create an informative title and change the author name to match your own For now, we will keep the file type as html. In the future, you can create pdf or .doc documents. However, these require additional programs installed on your computer. After you hit “OK” a new .Rmd script file will open in your top-left window with some template language and code chunks, similar to the image below. Save as .Rmd file. Save the file by going to “File -&gt; Save as” in RStudio Give the file an informative name like your LastnamePractice1.Rmd Key Components. Now you are ready to work within the Rmd script file. We will point to four basic components of this file, and you can build your knowledge of RMarkdown from there. The top part bracketed by --- on top and bottom is the YAML component. This tells RStudio the pertinent information about how to “compile” the Rmd file. Most of the time you can leave this alone, but you can always edit the title, author, or date as you wish. The next component are the global options for the document. It is conveniently labeled “setup.” By default what this is saying is that the compiled version will “echo” (i.e., display all code chunks and output) unless you specifically specify otherwise. For example, note that it says include = FALSE for the setup chunk. That setting means that this code chunk will “run” but it will not appear in the nicely compiled .html file. Most of the time you will not need to edit those settings. The third component I want to bring attention to is the body text. The # symbol in RMarkdown is used to indicate that you have a new section of the document. For example, in the compiled images at the beginning, this resulted in the text being larger and bolded when it said “Problem 2.” In addition to just using a single #, using ## or ### can indicate subsections or subsubsections. Other than that symbol, you can generally write text just as you would in any word processing program, with some exceptions, such as how to make text bold or italicized. (See bottom of section for resources on the Markdown language.) The final component I want to call attention to are the other main body code chunks. These are specific parts of the document where you want to create a mini R script. To create these, you can simply click the + C symbol toward the top of the top left window of RStudio and indicate you want an R chunk. For example, in the image above, there is an R code chunk labeled cars. The cars component is just a label for the code chunk. Labeling code chunks is not necessary. By default, a new R code chunk will just have r in the brackets, and that is sufficient. Writing R Code. Within a code chunk, you can type R code just like you would in any R script. To run (“execute”) the R code, you can run a single line the exact same way you do in a regular R script by moving the cursor to the end of a line of code or highlighting a portion of code and hitting “Run.” However, in RMarkdown, you also have the option of running an entire code chunk at once by hitting the green triangle at the top-right of a given code chunk. Knitting the document. Once you have added a code chunk and/or some text, you are ready to compile or “Knit” the document. This is what generates the .html document. To do so, click on the Knit button toward the top of the top-left window of Rstudio. After a few moments, this should open up a preview window displaying the compiled html file. It will also save an actual .html file in your working directory (the same location on your computer where you have saved the .Rmd file) Try to locate this compiled .html file on your computer and open it. For most computers, .html files will open in your default web browser, such as Google Chrome or Safari. This step is a common place where errors are detected and generated. Sometimes the compiling process fails due to errors in the R code in your code chunks or an error in the Markdown syntax. If your document fails to knit, the next step is to try to troubleshoot the error messages the compiling process generates. The best way to reduce and more easily detect errors is to “knit as you go.” Try to knit your document after each chunk of code you create. 1.3.2 Additional RMarkdown resources Here are a few additional resources for working with RMarkdown. This website provides some basic syntax for the Markdown language, such as how to display bulleted lists and how to bold or italicize text. This page walks through the setup of RMarkdown documents similar to what the course notes just did. This provides a second set of instructions and additional examples of settings you can use to customize your RMarkdown output (e.g., how large figures are when they are displayed). This page talks more about compiling aka rendering aka knitting Rmd documents into different formats, such as html, pdf, or Word doc files. 1.3.3 Practice with R Markdown Below is an exercise that will demonstrate you are able to use R as a calculator and compile RMarkdown documents. Create an Rmd file saved as “LastnameSetup1.Rmd” (use your last name). Provide an informative title for the document. Create a section labeled “Problems.” Create a code chunk where you do the calculation 8 + 3 - 2 in R. Store it as an object with an informative name. Report the answer as text underneath the code chunk. In a second code chunk, do the calculation 5 x 3 in R. Store it as an object with an informative name. Report the answer as text underneath the code chunk.. In a third code chunk, add these two calculations together. Note: do this by adding together the objects you created, not the underlying raw calculations. Report the answer as text underneath the code chunk.. Knit the file to create an html document. Open the html document in a web browser to check the formatting. "],["what-are-experiments.html", "1.4 What are experiments?", " 1.4 What are experiments? Our first discussion will be focused on elaborating on what we see as the goals of social science and how experiments fit into these goals. We draw on the following readings Gerber, A. and D.P. Green. 2012. Field Experiments: Design, Analysis, and Interpretation. W.W. Norton. Chapter 1. Angrist, Joshua D. and Jorn-Steffen Pischke. Mostly Harmless Econometrics. Part One: Preliminaries: “Questions about Questions.” Available online here Kinder, Donald R. and Thomas R. Palfrey. 1993. “On Behalf of an Experimental Political Science.” In Experimental Foundations of Political Science. Sen, Maya and Omar Wasow. 2016. “Race as a Bundle of Sticks: Designs that Estimate Effects of Seemingly Immutable Characteristics.” Annual Review of Political Science doi: 10.1146/annurev-polisci-032015-010015. We will sketch out the answers to these questions as a group. Along the way, we will try to build a research design for a research question we come up with as a class. p.comment { background-color: #DBDBDB; padding: 10px; border: 1px solid black; margin-left: 25px; border-radius: 5px; font-style: italic; } What are the goals of social science? What are examples of research questions that can be addressed with each goal? Your ideas … What makes an experiment an experiment? What are the goals of experimentation? Your ideas … What are some advantages of experimentation over other methods in political science? Your ideas … What are examples of different types of experiments? Your ideas … What are limitations of experiments? Can we experiment on everything? Your ideas … "],["causaleffects.html", "Section 2 Causal Effects", " Section 2 Causal Effects In this section, we discuss causal effects. It builds on Gerber and Green FEDAI Chapter 2. Goal of Causality Isolate the manipulation of one factor (“No causation without manipulation.”), while controlling or “holding everything else constant.” Does border security increase trust in government? Factual: Trust in an environment with border security Counterfactual: Trust in an environment without border security Does gender affect budgetary priorities? Factual: The budget under a village head who is male Counterfactual: The budget under a village head who is female Does race affect one’s job prospect? Factual: Jamal applied for a job but did not get it Counterfactual: Would Jamal have gotten a job if he were white? "],["potential-outcomes-framework.html", "2.1 Potential Outcomes Framework", " 2.1 Potential Outcomes Framework To make causal claims, we compare two states of the world and their potential outcomes: \\(Y_i(d)\\) What is \\(Y_i(0)\\)? What is \\(Y_i(1)\\)? \\(i\\) refers to individual subjects from \\(i = 1\\) to N. \\(d\\) is the treatment indicator \\(d_i\\) refers to whether the subject is treated: \\(d_i = 1\\) or \\(d_i = 0\\) \\(D_i\\) refers to a hypothetical treatment allocation A causal “treatment effect” is then the difference in these potential outcomes: \\(\\tau_i\\) = \\(Y_i(1)\\) - \\(Y_i(0)\\) FEDAI Table 2.1 The treatment effect is the difference between two states of the world: one which a unit receives treatment, and another in which it does not. 2.1.1 Average Treatment Effect The average treatment effect then is the mean of these individual treatment effects: Estimand: On average, how much outcomes would change if all units go from untreated to treated. \\[\\begin{align*} ATE &amp;= \\frac{1}{N} \\sum_{i=1}^N \\tau_i \\\\ &amp;= \\mu_{Y(1)} -\\mu_{Y(0)} \\\\ &amp;= \\frac{1}{N} \\sum_{i=1}^N Y_i (1) - \\frac{1}{N} \\sum_{i=1}^N Y_i (0) \\\\ &amp;= \\frac{1}{N} \\sum_{i=1}^N (Y_i (1)-Y_i (0))\\\\ &amp;= E[Y_i(1) - Y_i(0)]\\\\ \\end{align*}\\] ATE \\(= \\frac{1}{N} \\sum_{i=1}^N \\tau_i\\) is what we want to describe a causal effect, but in real life, we have problems. What are they? Try on your own, then expand for the answer. We only observe one potential outcome. \\(Y_i = d_iY_i(1) + (1-d_i)Y_i(0)\\) (Unless we are in Groundhog Day) "],["fundamental-problem-of-causal-inference.html", "2.2 Fundamental Problem of Causal Inference", " 2.2 Fundamental Problem of Causal Inference We only observe one potential outcome: \\(Y_i\\). \\(Y_i = d_iY_i(1) + (1-d_i)Y_i(0)\\) (Unless we are in Groundhog Day) Example from 2022 Dallas Cowboys game. We only get to observe \\(Y_i\\)= Cowboys lose. After the game, many people said things like: If the Cowboys had handed the ref the ball, \\(Y_i(1)\\) = Cowboys win If the Cowboys had continued to throw the ball instead of run, \\(Y_i(1)\\) = Cowboys win If Dak had just run a shorter distance instead, \\(Y_i(1)\\) = Cowboys win did this seriously just happen pic.twitter.com/MmUk8E1XSL — SB Nation (@SBNation) January 17, 2022 But the fundamental problem of causal inference is that we can only observe one potential outcome, the outcome in this case, under the state of the world \\(Y_i(0)\\) where the play unfolded as it did in the video. It is impossible to observe the actual causal effect of any of the above: \\(Y_i(1) -Y_i(0)\\) "],["identification-strategy.html", "2.3 Identification strategy", " 2.3 Identification strategy We cannot observe the ideal actual causal effect. Instead, we will frame our exercise on the premise that we are randomly sampling our \\(i&#39;s\\) from a population. We then will create an identification strategy. “Ideas that enable researchers to use observable quantities (e.g., sample averages) to reveal parameters of interest (e.g., average treatment effects)” (Gerber and Green 2012, 34) Instead of observing the actual individual causal treatment effect and actual ATE, we develop an estimator for this quantity using the sample averages. A few definitions: The sample average is a random variable, a quantity that varies from sample to sample.1 Expected value is the average outcome of a random variable weighted by its probability of occurrence. Good news: Under random sampling, the expected value of a sample average is the population average. Similarly, the expectation of a randomly selected observation from the population is the population mean. Even though we have a sample, under random sampling, our sample will be unbiased. On average, it’s true. When the expected value of a sample estimate is equal to the population parameter \\(E[\\hat{\\theta}] = \\theta\\), this means our estimator is “unbiased.” Expectation \\[\\begin{align*} E[X]=\\sum x Pr[X=x] \\end{align*}\\] where \\(Pr[X=x]\\) denotes the probability that \\(X\\) takes on the value \\(x\\), and where the summation is taken over all possible values of \\(x\\). Think of this like a weighted average. Example: \\(E[Y_i(1)]\\) is the expected value of the treated potential outcome of a subject who is randomly sampled.(It will equal the average value of all possible values.) What is the value of \\(E[Y_i(1)]\\) in this example? FEDAI Table 2.1 Note: other books may approach this slightly differently by defining a Sample ATE, taking \\(D_i\\) (treatment status) to be the random variable, and \\(Y_i(1)\\) as fixed within a sample. ↩︎ "],["difference-in-means-estimator.html", "2.4 Difference in Means Estimator", " 2.4 Difference in Means Estimator In the real world, we follow this process for causal identification: Our motivation: Find quantities that represent the population parameters (\\(\\theta\\)) Our problem: We often only get a sample of the population and can only observe one potential outcome for any unit in our sample Goal: Get unbiased estimators for the population Definition of unbiasedness: \\(E[\\hat{\\theta}] = \\theta\\) Suppose \\(D_i\\) were randomly assigned such that \\(m\\) subjects assigned to treatment and \\(N-m\\) subjects assigned to control. \\[\\begin{align*} \\widehat{ATE} &amp;= \\frac{1}{m}\\sum_1^m Y_i - \\frac{1}{N-m}\\sum_{m+1}^{N} Y_i \\\\ \\end{align*}\\] Is the difference in means estimator an unbiased estimate for the ATE? How can we find out? We take the expected value: \\[\\begin{align*} E[\\widehat{ATE}] &amp;= E[\\frac{1}{m}\\sum_1^m Y_i - \\frac{1}{N-m}\\sum_{m+1}^{N} Y_i ]\\\\ &amp;= \\frac{1}{m}\\sum_1^m E(Y_i) - \\frac{1}{N-m}\\sum_{m+1}^{N} E(Y_i ) \\\\ &amp;= \\frac{E(Y_1) + E(Y_2) +...+E(Y_m)}{m} - \\frac{E(Y_{m+1}) + E(Y_{m+2}) +...+E(Y_N)}{N-m}\\\\ &amp;= \\frac{m * E[Y_i(1 | D_i = 1)]}{m} - \\frac{(N-m)* E[Y_i(0) | D_i = 0]}{N-m}\\\\ &amp;= E[Y_i(1) | D_i = 1] - E[Y_i(0) | D_i = 0] \\\\ %&amp;= E[Y_i (1)]-E[Y_i (0)]=E[\\tau_i ]=ATE \\end{align*}\\] Is the final statement equivalent to the ATE? We want our final statement to be \\(E[Y_i (1)]-E[Y_i (0)]=E[\\tau_i ]\\)=ATE Our final statement is: \\(E[Y_i(1) | D_i = 1] - E[Y_i(0) | D_i = 0]=E[\\widehat{ATE}]\\) Under what conditions can we get those two statements to look the same? Well, let’s look into some rules of expectation. \\(E[Y|X] = E[Y]\\) if Y and X are independent.2 Our final statement can be simplified when treatment assignment is independent of potential outcomes: \\(E[Y_i(1) |D_i = 1] = E[Y_i(1) |D_i = 0] = E[Y_i(1)]\\) \\(E[Y_i(0) |D_i = 0] = E[Y_i(0) |D_i = 1] = E[Y_i(0)]\\) When does this occur? Random assignment of treatment!! Putting this together, under random assignment: \\[\\begin{align*} E[\\widehat{ATE}] &amp;= E[\\frac{1}{m}\\sum_1^m Y_i - \\frac{1}{N-m}\\sum_{m+1}^{N} Y_i ]\\\\ &amp;= \\frac{1}{m}\\sum_1^m E(Y_i) - \\frac{1}{N-m}\\sum_{m+1}^{N} E(Y_i ) \\\\ &amp;= E[Y_i(1) | D_i = 1] - E[Y_i(0) | D_i = 0] \\\\ &amp;= E[Y_i (1)]-E[Y_i (0)]=E[\\tau_i ]\\\\ E[\\widehat{ATE}] &amp;= ATE \\end{align*}\\] Why Experiments One approach for addressing the fundamental problem of causal inference is to simulate two potential states of the world through random assignment: Randomized Controlled Trials / Experiments Experiments approximate factual vs. counterfactual comparison We randomly assign one group to receive a “treatment” and another not to receive a treatment (the control) Using what we learned above, when treatment assignment is randomized, the only thing that distinguishes the treatment group from the control group in expectation, besides the treatment itself, is chance. This allows us to use a simple differences in means estimator in experiments to estimate our average treatment effects. See video for help on law of iterated expectations↩︎ "],["overview-of-identification-assumptions.html", "2.5 Overview of identification assumptions", " 2.5 Overview of identification assumptions What if we can’t guarantee random assignment? Example: Selection into treatment What if we didn’t have the independence? Subtract and add \\(E[Y_i (0) | D_i=1]\\) to help us illustrate a type of bias that may occur. \\(E[Y_i (1) | D_i=1]-E[Y_i (0) | D_i=0] =\\) \\(\\underbrace{E[Y_i (1) | D_i = 1] - E[Y_i (0) | D_i=1]}_{\\text{Average treatment effect for the treated}} + \\underbrace{E[Y_i (0)|D_i=1]-E[Y_i (0)| D_i=0] }_{\\text{Selection bias}}\\) In observational studies, where assignment into treatment is not random, the second term “Selection bias” may not be zero. E.g., suppose we want to know the effect of minimum wage laws on unemployment. Laws aren’t randomly assigned Possible that states where unemployment (outcome) is lower are less likely to see minimum wage laws passed relative to states where unemployment is higher. If so, the potential outcomes \\(Y_i(0)\\) of states that would hypothetically be treated or untreated would not be the same. Assumptions To “identify” the average treatment effect, we need Probability of treatment of all units is between 0 and 1 Ignorability: \\(Y_i(1), Y_i(0) \\perp D_i\\) (random assignment) Non-interference: \\(Y_i(d_1, d_2, ..., d_n) = Y_i(d)\\), \\(d_i = d\\) Excludability: if \\(Y_i(z, d)\\) where z \\(\\in [0, 1]\\) and \\(d \\in [0, 1]\\), \\(Y_i(1, d) = Y_i(0, d)\\) Let’s put these into plain words. "],["application-in-r.html", "2.6 Application in R", " 2.6 Application in R Article: “Are Emily and Greg More Employable Than Lakisha and Jamal? A Field Experiment on Labor Market Discrimination” by Bertrand and Mullainathan (2004) Research Question: Does race influence hiring decisions? What are the potential outcomes? What is the approach? Audit study: “send fictitious resumes to help-wanted ads in Boston and Chicago newspapers. Treatment: Manipulate perceived race: resumes randomly assigned African-American- or White-sounding names. Outcomes: Does the resume get a callback? How should we estimate the average treatment effect? 2.6.1 Loading the data We will use data from Imai (2017) Chapter 2. Let’s load the data. Note: When we have variables that are text-based categories, we may want to tell R to treat these “strings” of text information as factor variables, a particular type of variable that represents data as a set of nominal (unordered) or ordinal (ordered) categories. We do this with the stringsAsFactors argument. resume &lt;- read.csv(&quot;resume.csv&quot;, stringsAsFactors = T) resume &lt;- read.csv(&quot;https://raw.githubusercontent.com/ktmccabe/teachingdata/main/resume.csv&quot;, stringsAsFactors = T) Variables and Description firstname: first name of the fictitious job applicant sex: sex of applicant (female or male) race: race of applicant (black or white) call: whether a callback was made (1 = yes, 0 = no) The data contain 4870 resumes and 4 variables. nrow(resume) # number of rows ## [1] 4870 ncol(resume) # number of columns ## [1] 4 dim(resume) # number of rows and columns ## [1] 4870 4 head(resume) ## firstname sex race call ## 1 Allison female white 0 ## 2 Kristen female white 0 ## 3 Lakisha female black 0 ## 4 Latonya female black 0 ## 5 Carrie female white 0 ## 6 Jay male white 0 2.6.2 Variable classes We can check the class of each variable: Look, we have a new type, a “factor” variable. class(resume$firstname) ## [1] &quot;factor&quot; class(resume$sex) ## [1] &quot;factor&quot; class(resume$race) ## [1] &quot;factor&quot; class(resume$call) ## [1] &quot;integer&quot; Rules of Thumb Usually, we want character variables to store text (e.g., open-ended survey responses) We want numeric variables to store numbers. Usually, we want factor variables to store categories. Within R, factor variables assign a number to each category, which is given a label or level in the form of text. Categories might be ordinal or “ordered” (e.g., Very likely, Somewhat likely, Not likely) or Unordered (e.g., “male”, “female”) R won’t know if a factor variable is ordered or unordered. Alas, we have to be smarter than R. R might think you have a character variable when you want it to be a factor or the reverse. That’s when as.factor() and as.character() are useful. Always check class() to find out the variable type 2.6.3 Exploring Treatment and Control Groups We are going to use several different approaches to calculate our difference in means between treatment and control to help us explore R’s capabilities and common computational approaches. We can use the table command to see how many observations in our data fall into each category or numerical value. ## Example: how many black vs. white sounding resumes table(resume$race) ## ## black white ## 2435 2435 As mentioned, factor variables have levels: levels(resume$race) ## [1] &quot;black&quot; &quot;white&quot; We can also use the table command to show a crosstabulation: a table that displays the frequency of observations across two variables. Because our outcome variable call is dichotomous and we are interested in the rates of callbacks, we might use a table to display this information. (For outcomes that are continuous, the table approach is less useful.) ## Example: how many black vs. white sounding resumes by call backs ## We can label the two dimensions of the table with the = table(calledback = resume$call, race = resume$race) ## race ## calledback black white ## 0 2278 2200 ## 1 157 235 Sometimes we will want to show the proportion instead of the frequency using prop.table ## Example: proportion black vs. white sounding resumes by call backs ## Convert to proportion prop.table(table(calledback = resume$call, race = resume$race), margin = 2) # 1 for row sum, 2 for col ## race ## calledback black white ## 0 0.93552361 0.90349076 ## 1 0.06447639 0.09650924 How can we interpret this crosstabulation? 2.6.4 Means with Relational Operators Goal: Compare callback rates for white sounding names to black sounding names, so we need to be able to filter by race. Good news: We have several relational operators in R that evaluate logical statements: ==, &lt;, &gt;, &lt;=, &gt;=, != We have a statement and R evaluates it as TRUE or FALSE ## for each observation, does the value of race equal &quot;black&quot;? resume$race == &quot;black&quot; By putting this logical statement within [ ], we are asking R to take the mean() of the variable resume$call for the subset of observations for which this logical statement is TRUE. mean(resume$call[resume$race == &quot;black&quot;]) ## [1] 0.06447639 Ultimately, each of these paths has led us to a place where we can estimate the average treatment effect by calculation the difference in means: the difference in callback rates for black and white applicants. We said the ATE = \\(\\bar{Y}(treatment) - \\bar{Y}(control)\\) ate &lt;- mean(resume$call[resume$race == &quot;black&quot;]) - mean(resume$call[resume$race == &quot;white&quot;]) ate ## [1] -0.03203285 How can we interpret this? Do white applicants have an advantage? 2.6.5 Means with tidyverse The tidyverse offers a suite of R functions and a different grammar or syntax of coding. Some people prefer this to the “base R” codes we did above. To use this suite, first install the tidyverse package: When you install a package, this is like downloading an app to your phone. You only have to do it one time. install.packages(&quot;tidyverse&quot;) After you have a package installed, much like an app on your phone, you then need to open it before using it in R. To do so, use the library() command. library(tidyverse) The tidyverse works through these piping %&gt;% operators. We can read it from left to right. Take our dataset resume, group the data by race, and within each racial group, summarize the data by taking the mean call back rate. resume %&gt;% group_by(race) %&gt;% summarise(means = mean(call)) ## # A tibble: 2 x 2 ## race means ## &lt;fct&gt; &lt;dbl&gt; ## 1 black 0.0645 ## 2 white 0.0965 We could go a step further to calculate the ATE. ate &lt;- resume %&gt;% group_by(race) %&gt;% summarise(means = mean(call)) %&gt;% ungroup() %&gt;% spread(race, means)%&gt;% mutate(diff = black - white) ate ## # A tibble: 1 x 3 ## black white diff ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0645 0.0965 -0.0320 2.6.6 ATE with linear regression Linear regression also offers a way to calculate the conditional means and difference in means between two groups. In R, we use lm() for this. The syntax is lm(y ~ x, data = mydataframe). fit &lt;- lm(call ~ race, data =resume) We can look at the coefficient results only. fit$coefficients ## (Intercept) racewhite ## 0.06447639 0.03203285 In a regression of this form, the intercept represents the mean of the reference category, in this case, the callback rate for Black applicants. The coefficient on racewhite represents the difference in means between the reference category and this group. I.e., going from a Black applicant (the reference category) to a white applicant, on average, increases call backs by 3.2 percentage points. 2.6.7 Subsetting data in R Maybe we are interested in differences in callbacks for females. One approach for looking at the treatment effect for female applicants, only, is to subset our data to include only female names. To do this, we will assign a new data.frame object that keeps only those rows where sex == \"female\" and retains all columns Below are two approaches for this subsetting, one that uses brackets and one that uses the subset function ## option one females &lt;- resume[resume$sex == &quot;female&quot;, ] ## option two using subset()- preferred females &lt;- subset(resume, sex == &quot;female&quot;) Now that we have subset the data, this simplifies estimating the ATE for female applicants only. We said the ATE = \\(\\bar{Y}(treatment) - \\bar{Y}(control)\\) ate.females &lt;- mean(females$call[females$race == &quot;black&quot;]) - mean(females$call[females$race == &quot;white&quot;]) ate.females ## [1] -0.03264689 Question: Is this an unbiased estimate of the average treatment effect? Try on your own, then expand for the answer. This is an example of a “Conditional Average Treatment Effect.” Generally, because gender is a pre-treatment factor, we can condition on it and get unbiased estimates for the average treatment effect within a particular gender group. Random assignment of treatment means that in expectation, we should have about equal proportions of female applicants in each treatment group, ruling out the potential for selection bias. 2.6.8 Additional Practice We will use data from the article below: Thal, A. (2020). The desire for social status and economic conservatism among affluent Americans. American Political Science Review, 114(2), 426-442. In the experiment, affluent Americans are randomly assigned to encounter Facebook posts in which others broadcast their economic success. These posts are designed in a way that encourages affluent respondents to view economic success as a means of achieving social status. The experiment includes a sample of 2010 affluent Americans– people who report household incomes in the top 10 percent of the U.S. income distribution. Causal Question: Does desire for social status influence economic views of affluent Americans? Randomization: Randomly assign respondents to view different fictional Facebook posts designed to signal different motivations Outcome: An economic conservatism index based on respondents’ support for decreasing “taxes on households making $150,000 or more a year,” support for decreasing the “taxes on money people make from selling investments, also referred to as capital gains,” and support for decreasing “government regulation of business and industry.” Comparison: Average economic views between experimental conditions that vary in the type of social cues given. Let’s load the data! Here, note that the data file is in a .RData format instead of .csv. This means that instead of using read.csv, we should use a function to load the data that is suitable for the .RData format. This will be load. That function works the following way: load(&quot;status.RData&quot;) After running the above code, an object will show up in your R environment. head(status) ## condition male econcon ## 2 Concrete 1 0.7500000 ## 3 Self-Esteem 1 1.0000000 ## 4 Placebo 1 0.6666667 ## 5 Self-Esteem 0 0.2500000 ## 6 Self-Esteem 0 1.0000000 ## 7 Social Approval 0 0.8333333 The data include the following variables condition: Placebo, Concrete, Self-Esteem, Social Approval, Conspicuous Consumption gender: 1= male; 0= otherwise econcon: Economic views. Numeric variable from 0 to 1, with higher values reflecting more conservative views Practice: How many people are in each condition? What is the average treatment effect between the Placebo and Social Approval conditions? Try on your own, then expand for the answer. ## Number of observations table(status$condition) ## ## Placebo Concrete Conspicuous Consumption ## 394 391 392 ## Self-Esteem Social Approval ## 390 375 ## tidy groupmeans &lt;- status %&gt;% group_by(condition) %&gt;% summarise(means = mean(econcon)) %&gt;% ungroup %&gt;% spread(condition, means) groupmeans$`Social Approval` - groupmeans$Placebo ## [1] 0.05634969 ## relational operators ate &lt;- mean(status$econcon[status$condition == &quot;Social Approval&quot;]) - mean(status$econcon[status$condition == &quot;Placebo&quot;]) ate ## [1] 0.05634969 ## regression fit &lt;- lm(econcon ~ condition, data = status) fit$coefficients[&quot;conditionSocial Approval&quot;] ## conditionSocial Approval ## 0.05634969 Additional Review Questions What is this quantity \\(E[Y_i (1) − Y_i (0)]\\) conceptually? What is the fundamental problem of causal inference? How can we find out if our estimates are unbiased? (What process do we need to do?) With randomization, why is \\(E[Y_i (1)] = E [Y_i (1)|D_i = 1]\\)? What other assumptions do we need to estimate the ATE in an unbiased way using differences in means? "],["design.html", "Section 3 Experimental Design", " Section 3 Experimental Design In this section, we discuss best practices for experimental design, as well as implementing a design in Qualtrics. The two primary readings are: Mutz, D. (2021). Improving Experimental Treatments in Political Science. In J. Druckman &amp; D. Green (Eds.), Advances in Experimental Political Science (pp. 219-238). Cambridge: Cambridge University Press. Salganik, Matt. Chap 4 of Bit by Bit "],["designing-an-experiment.html", "3.1 Designing an Experiment", " 3.1 Designing an Experiment Four main ingredients in an experimental design Recruitment of participants Randomization of treatment - means people in treatment and control groups will be similar Delivery of treatment (intervention) Measurement of outcomes What does an experimental design test? Broadly “causal effects”: More specifically: From Mutz: Experiments are designed to answer the question, “If x changes, how should y be expected to change?” Goal of experimental treatment is to create variation in the independent variable in the direction (or directions) intended by the researcher. From Salganik: “Narrowly focused experiments answer a much more specific question: What is the average effect of this specific treatment with this specific implementation for this population of participants at this time?” How can we evaluate experiments? 3.1.1 Validity “Validity refers to the extent to which the results of a particular experiment support some more general conclusion.” Statistical conclusion validity- correctness of statistical analysis Internal validity- correctness of procedures Construct validity- match between data and theoretical constructs External validity- how can results generalize to other situations p.comment { background-color: #DBDBDB; padding: 10px; border: 1px solid black; margin-left: 25px; border-radius: 5px; font-style: italic; } What makes a good treatment? Does it require realism? Your ideas … How does our excludability assumption factor into this? Your ideas … What does it mean to say a treatment is generalizable? Your ideas … How can we increase engagement with our experiments? Your ideas … What is a manipulation check, and what role does it serve? Your ideas … 3.1.2 Design Space for Experiments Figure 4.1 p.comment { background-color: #DBDBDB; padding: 10px; border: 1px solid black; margin-left: 25px; border-radius: 5px; font-style: italic; } What are the tradeoffs between digital vs. analog experiments? Your ideas … What are the tradeoffs between lab vs. field experiments? Your ideas … 3.1.3 Types of Designs What are the strengths and weaknesses of different types of designs? Your ideas … "],["using-qualtrics.html", "3.2 Using Qualtrics", " 3.2 Using Qualtrics Everyone at Rutgers gets a free Qualtrics account. Qualtrics provides a user-friendly interface for designing online surveys and survey experiments. We will walk through how to design a simple survey experiment on the platform. Go to the Rutgers Qualtrics site. The first time you use this you might have to initialize your account. Click on “Create a new project” and select “Survey” from scratch and get started. Give the project an informative name like “Experimental methods demo.” We will start with a blank survey project. This should take you to a landing page that looks something like this: When running an academic survey, generally, the first survey question should be a consent form. Rutgers has template consent forms here. For a survey, you may want the Online survey questionnaire consent form. The text of the consent form has to be approved by the IRB. After the consent form, you might ask respondents a set of “pre-treatment” questions, such as demographics, attention checks, etc. These are things you want to know about respondents prior to when they enter your experiment. You can organize these questions into different “blocks.” Blocks make it easier to move groups of questions up and down the survey, randomize the order of questions people see within a given block, or branch people to see only one of a set of blocks. We will get to this later. Now we are ready to program an experiment. There are many ways to do this, but we will choose a couple of common approaches. In general, programming the experiment will involve 1) entering experimental treatments and questions into the survey interface we are currently working in; and 2) building a randomizer in the survey flow. To prepare to program your experiment, you should have a sense of how many unique experimental conditions you have. If you have a relatively small number of experimental conditions (e.g., 2-4), an easy way to program the experiment is to manually create a unique block for each condition. If you have a larger number of experimental conditions (e.g., if you are manipulating several things at once across conditions), you might consider integrating piped text or another approach to avoid the need to manually create all of your experimental conditions. If you have a very large number of conditions or need to adjust the randomization in a more complex way (e.g., control the specific probabilities that certain conditions appear), you may need to integrate JavaScript code to help with randomization. 3.2.1 Experimental Design with Vignette Experiment For this example, we will use the experimental design from “Public Opinion and Foreign Electoral Intervention” by Michael Tomz and Jessica Weeks, published in the American Political Science Review in 2020. The article is here. They “hypothesize that American tolerance of foreign intervention should depend on the type of intervention and the intended beneficiary. We distinguish three modes of interference: verbal endorsements, threats, and operations.” Endorsements occur when foreign countries express their opinions about candidates. Threats combine an endorsement with a promise of future reward or threat of future punishment, such as threatening to downgrade future relations if the preferred candidate loses. Operations [are] when foreign powers undertake efforts such as spreading embarrassing information about a candidate, hacking into voting systems, or donating money to an election campaign These are contrasted with a comparison of staying out of the election. Hypotheses “We predict that citizens will be most concerned about operations such as hacking into voting systems or donating money, as these directly advantage the favored candidate and involve behavior the U.N. has classified as impermissible interference in the internal affairs of another nation. Americans should be more tolerant of threats and most tolerant of endorsements, which could be seen as legitimate and harmless expressions of opinion that do not intrude on American sovereignty.” “We also hypothesize that revelations of foreign intervention will generate polarized partisan responses. . . we anticipate that American voters will disapprove more strongly of foreign meddling on behalf of political opponents, than of foreign meddling to assist their own party.” Table 1 in the paper displays the experimental design used to test the hypotheses. We will program the primary manipulation, which varies the endorsement, threat, operation, or stay out conditions. For now, we will fix the country to be China, the candidate to be the Democratic candidate, and the operation to be donating money and 100% certainty it was China. 3.2.2 Unique blocks per experimental condition In this approach, we will create a separate block for each unique experimental vignette. In our case, we will create a block with a Text/Graphic question type. We paste in our experimental text and give the block and question informative labels. The question name will be the name of the variable for the question when you eventually load the data. For each condition, create a new block. See the four blocks below, one for each condition After creating each experimental block, we can then add a new block with our outcome questions. Go ahead and add 1-2 outcome questions so that you have an example.If your outcome condition text is specific to each treatment condition, you can create outcome questions within the experimental blocks. Finally, after that, you may have some last demographic questions. You can put those in yet another last block. Once you have created all your blocks, you can now go to the survey flow. To do that, click on the icon in the left side of the survey landing page. Your survey flow should look something like this. We are now ready to add randomization so that each respondent only sees one of our experimental blocks, randomly assigned. 3.2.3 Adding a randomizer in the survey flow Within the survey flow, add a randomizer underneath the consent form. Under the randomizer, add an embedded data field with an informative label for your treatment (e.g., “treat”) Create a unique value for each of your treatment conditions Make sure the randomizer is set to show just one of these values. As people go through the survey, under the hood of qualtrics, they will be assigned one of your experimental condition values. This embedded data field will show up as a variable in the data you download. However, we need one more step to make sure people only see the experimental block that corresponds to their embedded data field. This works through branching. Above each experimental block, add a “Branch” object Branch people using embedded data. Set the condition to match each embedded data value, and then move the experimental blocks underneath the appropriate branch. You can also duplicate the outcome block and put them underneath the corresponding treatment blocks. Hit apply to make sure the survey flow saves. A last thing we often want to do is add a branch underneath the consent form to end the survey for anyone who does not consent to take the survey. The specific end-of-survey message you provide may be different depending on which company you use to recruit respondents. For now, we will use a default end-of-survey message. You are now ready to “Preview” your survey! 3.2.4 Using Piped Text in Randomization We could complicate the randomization more so than we have done so far. For example, in Tomz and Weeks (2020), they do not fix the country to be China. Instead, they randomly vary this to be China, Pakistan, or Turkey for each respondent. We could build this added treatment arm into our design through “piped text.” Go back into your survey data flow. Create a second randomizer towards the top of the survey. Create a new embedded data field with an informative label, like country. Create unique fields for each of our country names. Hit apply. Now, in addition to being assigned a treatment condition, everyone is also independently randomly assigned a country. We now need to make sure the text they see reflects both their treatment condition and country. We could create 4 X 3 experimental blocks to reflect these dual randomizations. Instead, we are going to integrate the second treatment into the four blocks we have already programmed– just to save us time. Back in the survey landing page, click on every single block of text or question that includes the word “China.” In place of “China” click on the “piped text” option. Set the piped text to be the “country” embedded data field. Here is an example. Note: you need to do this for every single mention of country. We could complicate our design even further by adding additional piped text randomization to vary whether it is the Democratic vs. Republican candidate, the percent certainty about the country involved, and the type of operation. This would all involve adding additional randomizers and/or branching in the survey flow, along with piped text in the experimental blocks. 3.2.5 Data and Analysis Once you have a draft of your survey programmed, you will want to “preview” the survey from the perspective of a respondent by clicking “preview” in the survey landing page. Repeat this a few times to see if things seem to be working properly. After that, you can do a few more steps to test your survey. Option 1: Fake Data. In the survey landing, go to Tools -&gt; Generate test responses. This will automatically generate fake/simulated responses to your survey. This can allow you to check how the data will download, see if you can load it into your preferred statistical software and access the variables in the way you imagined, and check if the randomization appears to work properly. Option 2: Get a distribution link for your survey, and send the link to your friends and colleagues to help you test the survey from a respondent’s perspective. Go to Distributions -&gt; Get a single reusable link Note: once you click this, your survey is now published and “active.” To make any future changes to your survey, on the survey landing you will have to click “Publish.” With both of these options, your survey will start to populate responses in the Data and Analysis section of Qualtrics. Click on this now that you have done one or both of these steps. This will give you an overview of the responses and number of recorded responses. This is also where we can download the data. Go to Export and Import - &gt; Export Data. Download the data as a csv file if you plan to work in R. If you click on “More options,” you can export randomized viewing order as well as other features you may want to toggle on or off. If you open up your .csv file in a spreadsheet software like Excel, you will notice that the first row contains your question names as variables. The next two rows are more information about the questions, including the question wording. The actual responses start in row 3. If you load the csv file into R as is, you would want to delete the first two rows from your dataframe (the first row will automatically be treated as a header in R) Alternatively, you can delete rows 2-3 from the spreadsheet software before loading it into R. Save the .csv file with an informative name in the working directory where you store files to work on in R. Load the data into R. ## my data are in a /data/ subfolder of my working directory expdemo &lt;- read.csv(&quot;data/expdemo.csv&quot;) expdemo &lt;- expdemo[-c(1:2),] # remove first two rows Let’s limit the sample to those who agreed to our consent form. Locate your consent variable and subset on those who agree. expdemo &lt;- subset(expdemo, QID1 == &quot;I Agree&quot;) At this stage, we just have fake/test respondents. However, we can still see if the randomization works properly and if the outcome questions are populating in the way we want. For example, we want about a quarter of respondents assigned to each of the experimental conditions. table(expdemo$treat) ## ## endorsement operation stayout threat ## 71 58 57 56 And let’s make sure our outcomes are populating correctly. Note how people from each condition have populated the outcomes. This gives us confidence that the survey logic is working correctly. If, for example, no one from the endorsement condition had answered the outcome, this might mean we had a typo or other error in our survey logic. table(expdemo$approval) ## ## Approve somewhat Approve strongly ## 51 46 ## Disapprove somewhat Disapprove strongly ## 49 43 ## Neither approve nor disapprove ## 53 table(condition=expdemo$treat, outcome = expdemo$approval) ## outcome ## condition Approve somewhat Approve strongly Disapprove somewhat ## endorsement 13 15 19 ## operation 13 9 9 ## stayout 15 11 7 ## threat 10 11 14 ## outcome ## condition Disapprove strongly Neither approve nor disapprove ## endorsement 13 11 ## operation 9 18 ## stayout 11 13 ## threat 10 11 If our survey programming was all set, at this point, you could actually set up your entire R code and analysis based on the fake data. That would mean that all you have to do after you run the survey with real respondents is switch the dataset you load into the software. That would be the ultimate “pre-analysis plan.” Once you are done testing in Qualtrics, back in the Data and Analysis page, you can delete all responses using Tools -&gt; Delete data. Once you are done testing and revising the survey, you are now ready to integrate it with your preferred survey firm/recruiting platform. The specific steps from here going forward vary across platforms. 3.2.6 Additional Bells and Whistles Qualtrics has a number of other features you can use, including different question types, the ability to randomize the order of response options, features to require/request responses.You can continue to explore these as you develop your own surveys. Their help pages are pretty useful. Here is one on question types. For those familiar with “conjoint experiments” that have a lot of randomization, Anton Strezhnev has developed a tool for programming these in Qualtrics. See information here. It is also possible to download data from Qualtrics directly into R using an R package here. "],["uncertainty.html", "Section 4 Uncertainty", " Section 4 Uncertainty In this section, we cover calculations of uncertainty following Gerber and Green Chapter 3 and 9.3. We cover the computation of standard errors and confidence intervals. We introduce the null hypothesis testing framework. We then look at an experimental application and compute t-tests and interactions to assess average and heterogeneous treatment effects. Finally, we examine randomization inference as an alternative to t-tests. "],["standard-errors.html", "4.1 Standard Errors", " 4.1 Standard Errors Standard errors represent the standard deviation of a sampling distribution. What is the standard deviation? Measure of spread: typical deviation of an observation from the mean. From the Cartoon Guide to Statistics To calculate the standard deviation: Take a squared deviation from the mean for a unit \\(i\\). \\[\\begin{align*} &amp;= (y_i - \\bar{y})^2 \\end{align*}\\] Do this for each unit \\(i\\) out of a sample. Take the sum. \\[\\begin{align*} &amp;= \\sum_{i=1}^{N} (y_i - \\bar{y})^2 \\end{align*}\\] Divide over the total sample. When we are dealing with a sample from a population whose mean is unknown (usually the case), we have to take N-1 instead of N. \\[\\begin{align*} &amp;= \\frac{1}{N-1} \\sum_{i=1}^{N} (y_i - \\bar{y})^2 \\end{align*}\\] Take the square root \\[\\begin{align*} s &amp;= \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^{N} (y_i - \\bar{y})^2 } \\end{align*}\\] Remember: standard deviation is the square root of the variance! Let’s say these were the data for a sample of 10 voters’ scores on a feeling thermometer of their views toward liberals. fts &lt;- c(40, 95, 100, 5, 75, 80, 65, 100, 90, 28) ## 1. take the squared deviation from the mean sq.dev &lt;- (fts - mean(fts))^2 ## 2. Take the sum sum.sq.dev &lt;- sum(sq.dev) ## 3. Divide over N - 1 sum.sq.dev.n &lt;- sum.sq.dev/(length(fts) - 1) ## 4. Take the square root s &lt;- sqrt(sum.sq.dev.n) s ## [1] 33.02457 ## optional code sd(fts) # or sqrt(var(fts)) ## [1] 33.02457 What is a sampling distribution? The experiment we happen to conduct yields an estimate of the average treatment effect, but in a different randomization, our estimate might have been different. Sampling distribution refers to the set of estimates that could have been generated by every possible random assignment. The standard error is a measure of the spread of this distribution. Good news: Under the central limit theorem, this distribution approximates the shape of a normal distribution when there are sufficient observations. Why is this good news? Going to help us estimate uncertainty. 4.1.1 Standard Error of the Mean The population mean and variance are \\(\\mu_y\\) and \\(\\sigma^2_y\\) for some variable \\(Y\\) and \\(\\sigma_y\\) is the standard deviation. We want to know the variability of our sample mean \\(\\bar{Y}\\). Well we already know the mean of our sample mean (\\(\\bar{Y}\\)) is the population mean \\((\\mu_y)\\): \\[\\begin{align*} E(\\bar{Y}) &amp;= E[\\frac{1}{N}\\sum_{i=1}^{N} y_i] \\\\ &amp;= \\frac{1}{N}*[E(y_1) + E(y_2) + ... + E(y_N)]\\\\ &amp;= \\frac{1}{N}*N\\mu_y\\\\ &amp;= \\mu_y \\end{align*}\\] What about the variance of \\(\\bar{Y}\\)? We call the variance of our population mean: \\(\\sigma^2\\). \\[\\begin{align*} Var(\\bar{Y}) &amp;= Var[\\frac{1}{N}\\sum_{i=1}^{N} y_i] \\\\ &amp;= \\frac{1}{N^2}*[Var(y_1) + Var(y_2) + ... + Var(y_N)]\\\\ &amp;= \\frac{1}{N^2}*N\\sigma^2\\\\ &amp;= \\frac{\\sigma^2}{N} \\end{align*}\\] Then to get to the standard error, we take the square root: \\[\\begin{align*} &amp;= \\frac{\\sigma}{\\sqrt{N}} \\end{align*}\\] We cannot observe the actual \\(\\sigma\\) so instead, we will follow the practice of using a “sample analogue.” In our case, this is \\(s\\), the sample standard deviation: So we have an estimate for the standard error of our sample mean: \\[\\begin{align*} \\widehat{SE}_m &amp;= \\frac{s}{\\sqrt{N}}\\\\ \\end{align*}\\] Computing the estimate of our standard error Take the standard deviation of our sample. Recall: \\[\\begin{align*} s &amp;= \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^{N} (y_i - \\bar{y})^2 }\\\\ \\end{align*}\\] Divide by the square root of the sample size. \\[\\begin{align*} \\widehat{SE}_m &amp;= \\frac{s}{\\sqrt{N}}\\\\ \\end{align*}\\] Example What’s our estimate for the mean and standard error for feeling thermometer scores toward liberals? ## 1. Take standard deviation st.dev.fts &lt;- sd(fts) ## 2. Divide by square root of sample size se.fts &lt;- st.dev.fts/sqrt(length(fts)) se.fts ## [1] 10.44329 ## Alternative sqrt(var(fts)/length(fts)) ## [1] 10.44329 ## What&#39;s the mean? mean(fts) ## [1] 67.8 4.1.2 Standard error for a difference in means So we have an estimate for the standard error of our sample mean: But often we want the standard error of a difference in means, correspondig to the uncertainty of \\(\\widehat{ATE}\\). When we take the difference in variances from two independent samples, we add their variances: \\[\\begin{align*} \\widehat{SE}_{d-i-m} &amp;= \\sqrt{\\frac{\\widehat{Var}(Y_i(1))}{m} + \\frac{\\widehat{Var}(Y_i(0))}{N-m}}\\\\ \\end{align*}\\] Note: Let’s inspect this formula. What does it tell us about when the standard error will be larger/smaller? Gives us insight into designs with blocking, matched pairs "],["confidence-intervals.html", "4.2 Confidence Intervals", " 4.2 Confidence Intervals Take a sample statistic (e.g.,\\(\\bar{Y}\\)) Set a test value. A common one is \\(\\alpha = 0.05\\) Find the critical value associated with the test level. Example: \\[\\begin{align*} z_{crit (1-\\alpha/2)} &amp;= 1.96\\\\ \\end{align*}\\] Multiply the critical value by the standard error of your statistic, and add and subtract from the statistic \\[\\begin{align*} CI &amp;= \\bar{Y} +/- crit.value*\\widehat{SE}_{\\bar{Y}}\\\\ \\end{align*}\\] Careful when interpreting CI’s: note that the interval may vary from experiment to experiment, while the parameter stays fixed. Example computing confidence intervals m.fts &lt;- mean(fts) ## What&#39;s our test level? .05 alpha &lt;- .05 ## critical value for normal distribution crit.z &lt;- qnorm(1- alpha/2) ## critical value for t-distribution crit.t &lt;- qt(1- alpha/2, df = (length(fts)-1)) ## Confidence interval using t-distribution ci.ub &lt;- m.fts + crit.t*se.fts ci.lb &lt;- m.fts - crit.t*se.fts c(ci.lb, ci.ub) ## [1] 44.17565 91.42435 ## Alternative using the R t.test function t.testfts &lt;- t.test(fts) t.testfts$conf.int[1:2] ## [1] 44.17565 91.42435 "],["hypothesis-tests.html", "4.3 Hypothesis Tests", " 4.3 Hypothesis Tests Generally, we want to actually test hypotheses. We will use the null hypothesis testing framework. In this framework, we collect evidence to reject or fail to reject a naive starting assumption: the null hypothesis. Typical setup for two-sample test. Null hypotheis: \\(H_o\\): \\(\\mu_{Y(1)} = \\mu_{Y(0)}\\) Alternative hypothesis: \\(H_a\\): \\(\\mu_{Y(1)} \\neq \\mu_{Y(0)}\\); or \\(H_a\\): \\(\\mu_{Y(1)} &gt; \\mu_{Y(0)}\\) or \\(H_a\\): \\(\\mu_{Y(1)} &lt; \\mu_{Y(0)}\\) Review: Let’s say we do a two-sided test and get a p-value from our t-test of 0.003. What should we conclude? Wait, what’s a p-value? How should we interpret this p-value? (pg. 64, Gerber and Green) 4.3.1 t-tests A common implementation of hypothesis tests for comparing averages of two groups is the t-test. Single population \\[\\begin{align*} t &amp;= \\frac{\\bar{X} - \\mu_o}{\\widehat{SE}_m}\\\\ \\end{align*}\\] Two populations \\[\\begin{align*} t &amp;= \\frac{(\\bar{X_1} - \\bar{X_0})- (\\mu_1 - \\mu_0)}{\\widehat{SE}_{d-i-m}}\\\\ \\end{align*}\\] In each case, we standardize our estimates according to the student’s t-distribution. We look to see just how extreme our t statistic is. t is our test statistic, a ratio between the size of the difference in means over the variability in the underlying data, represented by the standard error. Here is a relatively accessible summary of t values. 4.3.2 p-values The p-value asks: What is the probability of getting a result this extreme or more extreme “by chance”/“if the null were true”? In a world where the null is true, we still might not get a t=0 in every sample. The t-distribution represents the range of t-values we might expect to see with some probability under the assumption the null is true. We need to quantify how likely it would be to get our t-statistic in this world where the null is true. Lower-tailed test, p-value \\(= Pr(T &lt; t | H_o\\) is true) Upper-tailed test, p-value \\(= Pr(T &gt; t | H_o\\) is true) Two-sided test is specified by: p-value \\(= 2 * P(T &gt; |t| \\hspace{1mm} | H_o\\) is true) We primarily use two-sided tests. To get the p-value, we need the degrees of freedom because the t-distribution varies somewhat in shape according to the degrees of freedom, which are primarily a function of the sample size. Degrees of freedom govern how thick the tails of the distribution are, which can influence and increase the size of the p-value. For one sample tests, it is N-1. For two-sample t-tests, the degrees of freedom calculation can be more complicated. If we use the Welch calculation for unequal variances, which is the default setting in the R t.test function it is: df\\(=\\frac{\\widehat{SE}^4}{ \\frac{\\widehat{Var}(Y_i(1))^2}{m^2(m-1)} + {\\frac{\\widehat{Var}(Y_i(0))^2}{(N-m)^2(N-m-1)}}}\\). Fortunately, the t.test function in R will calculate that for you. "],["empirical-application.html", "4.4 Empirical Application", " 4.4 Empirical Application We will use data from the following experiment: “Social Exclusion and Political Identity: The Case of Asian American Partisanship” by Alexander Kuo, Neil Malhotra, and Cecilia Mo (2016). The data set based on authors’ replication file here Research Question: Do feelings of social exclusion lead Asians to develop more negative feelings toward the Republican Party? Sample: 114: 61 self-reported Asian, 53 self-reported white Treatment: Manipulate feelings of social exclusion. Outcome: Difference in views toward Democratic vs. Republican Party Close-mindedness, ignorance, represent interests, likes/dislikes, feeling thermometer, party ID, and the average of these six Let’s put this in the potential outcomes framework. For a given unit \\(i\\) what are the potential outcomes we are interested in? What is the \\(\\tau_i\\) we are interested in? How are we going to estimate it? 4.4.1 Treatment For those in the treatment group, a white female assistant to the research team says, “I’m sorry; I forgot that this study is only for US citizens. Are you a US citizen? I cannot tell.” If the subject was a US citizen, the assistant was instructed to say “OK, go ahead” and have the respondent start the survey; if the subject was not a US citizen, the assistant was instructed to pause and then say “it’s OK, go ahead.” Subjects then completed an online survey of their political attitudes. p.comment { background-color: #DBDBDB; padding: 10px; border: 1px solid black; margin-left: 25px; border-radius: 5px; font-style: italic; } Is this treatment a good treatment? Use the principles we discussed last section to evaluate this implementation. Your ideas … 4.4.2 Analysis Let’s load the data. library(foreign) exclusion &lt;- read.dta(&quot;data/exclusion.dta&quot;) ## Explore data ## How many subjects? ## How many Asian vs. White subjects ## What proportion of subjects were treated? ## Let&#39;s relabel the names to something sensible names(exclusion) ## [1] &quot;v1&quot; &quot;v2&quot; &quot;v3&quot; &quot;v4&quot; ## [5] &quot;v5&quot; &quot;v6&quot; &quot;study2_avg&quot; &quot;treatment_cit&quot; ## [9] &quot;asiant&quot; ## v1 is difference between dem - rep in closed mindedness names(exclusion)[1] &lt;- &quot;clmindeddr&quot; ## v2 is difference between dem - rep in ignorance names(exclusion)[2] &lt;- &quot;ingnorantdr&quot; ## What if you don&#39;t want to have to find the number? names(exclusion)[names(exclusion) == &quot;v3&quot;] &lt;- &quot;netlikesdr&quot; names(exclusion)[4] &lt;- &quot;piddr&quot; # pid names(exclusion)[5] &lt;- &quot;ftdr&quot; # feeling thermometer names(exclusion)[6] &lt;- &quot;repdr&quot; # represents interests ## Difference in means for the average ## Overall d.i.m &lt;- mean(exclusion$study2_avg[exclusion$treatment_cit == 1]) - mean(exclusion$study2_avg[exclusion$treatment_cit == 0]) ## Among whites diff.whites &lt;- mean(exclusion$study2_avg[exclusion$treatment_cit == 1 &amp; exclusion$asiant == 0]) - mean(exclusion$study2_avg[exclusion$treatment_cit == 0 &amp; exclusion$asiant == 0 ]) ## Among asians diff.asians &lt;- mean(exclusion$study2_avg[exclusion$treatment_cit == 1 &amp; exclusion$asiant == 1]) - mean(exclusion$study2_avg[exclusion$treatment_cit == 0 &amp; exclusion$asiant == 1 ]) We could also subset the data by race/ethnicity group. Let’s do that and then calculate our t-test and uncertainty by hand and using the R functions. ## Subset data for only Asian respondents asians &lt;- subset(exclusion, asiant == 1) ## t-test by hand for sample of Asian Respondents ## Calculating Standard error ## Get N for each group n.asianst1 &lt;- length(asians$study2_avg[asians$treatment_cit == 1]) n.asianst0 &lt;- length(asians$study2_avg[asians$treatment_cit == 0]) ## Get variance for each group v.asianst1 &lt;- var(asians$study2_avg[asians$treatment_cit == 1]) v.asianst0 &lt;- var(asians$study2_avg[asians$treatment_cit == 0]) ## Standard error se.diffasians &lt;- sqrt(v.asianst1/n.asianst1 + v.asianst0/n.asianst0) ## t-statistic t.diffasians &lt;- diff.asians/se.diffasians ## Degrees of freedom t.df &lt;- (se.diffasians)^4/ (v.asianst1^2/(n.asianst1^2*(n.asianst1 - 1)) + v.asianst0^2/(n.asianst0^2*(n.asianst0 - 1))) ## p-value for two-sided test p.asians &lt;- (1- pt(abs(t.diffasians), t.df))*2 We could visualize this according to the t-distribution with degrees of freedom equal to t.df: 50.73481 and our t-value of 2.196597 in the dashed red line. To get our p-value in a two-sided test, we compute the area to the right of this and to the left of its corresponding value on the opposite side of the distribution (equivalently due to the symmetric nature of the distribution, we can take 2 \\(\\times\\) either area). This area represents a probability, as the total area under the curve sums to 1. A shortcut for computing the results is to use the R function. When learning a new function, you can access the help files in R by typing ?FUN into the console. Example: t.test. ## t-test the quick way! asians.t &lt;- t.test(asians$study2_avg[asians$treatment_cit == 1], asians$study2_avg[asians$treatment_cit == 0]) whites &lt;- subset(exclusion, asiant == 0) whites.t &lt;- t.test(whites$study2_avg[whites$treatment_cit == 1], whites$study2_avg[whites$treatment_cit == 0]) What are our conclusions about the hypothesis tests? 4.4.3 Heterogeneous Treatment Effects The researchers believe that the size of the treatment effects will be different depending on the race/ethnicity of the participant. Should we compare the treatment effects among Asians vs. whites? If we do, can we say that being Asian caused a different reaction to microaggressions than being white? Overall, what are the limits of studying heterogeneity? One approach to detecting a heterogeneous treatment effect is to use an interaction in a linear regression model. As we discussed in the second section, when you have a treatment categorical variable, the regression coefficient \\(\\hat \\beta\\) represents the difference in means. When we interact this treatment indicator with a second variable, it will tell us how much the treatment effect varies according to the levels of that second variable. Let’s start by calculating our treatment effects with the regression approach. ## Linear regression lm(y ~ x, data = nameofyourdataframe) asians.r &lt;- lm(study2_avg ~ treatment_cit, data = asians) summary(asians.r) whites.r &lt;- lm(study2_avg ~ treatment_cit, data = whites) summary(whites.r) Why is the p-value slightly different here? Try on your own, then expand for the answer. We do not assume our groups have equal variances when we do the t-test, but regression relies on a pooled variance estimator, which differs slightly. We can recover the p-value in two ways. ## Indicate var.equal=T asians.t.p &lt;- t.test(asians$study2_avg[asians$treatment_cit == 1], asians$study2_avg[asians$treatment_cit == 0], var.equal = T) asians.t.p$p.value ## [1] 0.02951252 ## By hand, use pooled estimator for variance/standard error/degrees of freedom pooled.var &lt;- ((n.asianst1 - 1) * v.asianst1 + (n.asianst0 - 1)*v.asianst0)/ (n.asianst0 + n.asianst1 -2) pooled.se &lt;- sqrt(pooled.var) * sqrt( 1/n.asianst0 + 1/n.asianst1) pooled.t &lt;- diff.asians/pooled.se pooled.p &lt;- (1- pt(abs(pooled.t), (n.asianst0 + n.asianst1 -2)))*2 pooled.p ## [1] 0.02951252 Let’s now add the interaction term using the asterisk symbol: ## Using an interaction het.r &lt;- lm(study2_avg ~ treatment_cit + asiant + treatment_cit*asiant, data = exclusion) summary(het.r) ## ## Call: ## lm(formula = study2_avg ~ treatment_cit + asiant + treatment_cit * ## asiant, data = exclusion) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.52680 -0.07572 0.02925 0.09661 0.32821 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.67267 0.02976 22.606 &lt;2e-16 *** ## treatment_cit -0.03200 0.04517 -0.708 0.4802 ## asiant -0.07654 0.04244 -1.803 0.0741 . ## treatment_cit:asiant 0.12517 0.06153 2.034 0.0443 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.163 on 110 degrees of freedom ## Multiple R-squared: 0.04907, Adjusted R-squared: 0.02314 ## F-statistic: 1.892 on 3 and 110 DF, p-value: 0.1351 ## How do we interpret each coefficient? We now have a t-test for the interaction term, specifically. Be mindful that when you include an interaction term in a regression model, it changes the way we interpret the two other “main effects.” 4.4.4 Difference in Proportions If we have a proportion as an average outcome instead of a mean, we may wish to adjust how we calculate uncertainty to better reflect the nature of a dichotomous outcome variable. When we are comparing two proportions, we can use the prop.test function in R, which will adjust this calculation for us. ## Create a proportion variable where 1=dem, 0=rep table(exclusion$piddr) ## ## 0 0.200000002980232 0.400000005960464 0.600000023841858 ## 2 7 12 28 ## 0.800000011920929 1 ## 35 30 asians$dem &lt;- ifelse(asians$piddr &gt; .5, 1, 0) ## Calculate difference in proportions by hand m1.asians &lt;- mean(asians$dem[asians$treatment_cit == 1]) m0.asians &lt;- mean(asians$dem[asians$treatment_cit == 0]) m1.asians - m0.asians ## [1] 0.1163793 ## Use prop.test: NOTE THE DIFFERENT SYNTAX FROM t.test ## x is the &quot;number of successes&quot; i.e., number of 1&#39;s for each group ## n is sample size for each group p.test.asians &lt;- prop.test(x = c(sum(asians$dem[asians$treatment_cit == 1]), sum(asians$dem[asians$treatment_cit == 0])), n = c(length(asians$dem[asians$treatment_cit == 1]), length(asians$dem[asians$treatment_cit == 0]))) ## Note if you were to run the standard t-test, ## the difference would be the same but calculation of uncertainty is different t.test(asians$dem[asians$treatment_cit == 1], asians$dem[asians$treatment_cit == 0]) ## ## Welch Two Sample t-test ## ## data: asians$dem[asians$treatment_cit == 1] and asians$dem[asians$treatment_cit == 0] ## t = 1.1599, df = 52.547, p-value = 0.2514 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.08491716 0.31767578 ## sample estimates: ## mean of x mean of y ## 0.8750000 0.7586207 What do you conclude about the test? "],["randomization-inference.html", "4.5 Randomization Inference", " 4.5 Randomization Inference Null Hypothesis of No Average Effect vs. Sharp Null Hypothesis of No Effect (pg. 62) \\(\\mu_{Y(1)} = \\mu_{Y(0)}\\) vs. \\(Y_i(1) = Y_i(0)\\) for all i What are the key differences here? The treatment has no effect: \\(Y_i(1) = Y_i(0)\\) for all \\(i\\). Suppose we are in the world where the sharp null is true. Let’s simulate what the sampling distribution under that null distribution looks like. We assess the distribution relative to the ATE we observe under the assignment in our sample How likely is it we would observe our ATE, given the null distribution? Example Here is our data for 7 observations, where 4 are assigned to treatment, 3 to control. Our estimate for the average treatment effect is \\(6-3 = 3.\\) Suppose the sharp null is true: \\(Y_i(1) = Y_i(0)\\). This means \\(\\tau_i\\) = 0 for all \\(i.\\) In our null world, if we know \\(Y_i\\) for each \\(i\\) and \\(\\tau_i\\) for each \\(i\\), we can solve for the missing potential outcome. For randomization inference, what we do now is simulate possible randomizations– what if a different set of observations were treated each time? What is our average treatment effect for d? 18/4 - 15/3 = -.5 Repeat for all (or a lot of) possible permutations of d. This gives us an implied null distribution of the average treatment effect under the sharp null. Note: it won’t always be zero. It will be a distribution around zero. We will compare how extreme our observed estimate of the average treatment effect is compared to this distribution under the null. Empirical example with social exclusion experiment Alex Coppock has updated the randomization inference package to ri2 in R. More on this package is available here. ## install.packages(&quot;ri2&quot;, dependencies=T) library(ri2) ## Declare randomization declaration &lt;- declare_ra(N=nrow(asians), prob=.5) ## Estimate the average treatment effect set.seed(1215) ri2_out &lt;- conduct_ri( formula = study2_avg ~ treatment_cit, assignment = &quot;treatment_cit&quot;, declaration = declaration, sharp_hypothesis = 0, data = asians ) Plot and compare distribution to observed ATE plot(ri2_out) Summarize Output summary(ri2_out) ## term estimate two_tailed_p_value ## 1 treatment_cit 0.09317187 0.033 We can manually see what the package is doing by counting how many of the simulated estimates from the empirical distribution of the sharp null hypothesis were more extreme than our estimate from the study. Note that in this case, our p-value is very similar to the t-test. estimate &lt;- tidy(ri2_out)$estimate nsims &lt;- length(ri2_out$sims_df$est_sim) simstimates &lt;- ri2_out$sims_df$est_sim ## Two-tailed p-value length(simstimates[abs(simstimates) &gt;= abs(estimate)])/nsims ## [1] 0.033 "],["a-note-on-expectations-and-variance.html", "4.6 A Note on Expectations and Variance", " 4.6 A Note on Expectations and Variance This section includes definitions of expectation and variance and walks through more deliberately how we derive what the variance of a mean is (which we need for standard errors!). This video walks you through the steps in the slides from Week 3 Uncertainty with very similar notation. Below, we go through a more elongated version of the derivation that starts from the basic definition of the variance of a random variable. We are going to exploit a few “rules” of expectations and variance as we go through the derivation. The expectation of a sum is the sum of the expectations: \\(E(X + Y) = E(X) + E(Y)\\) The expectation of a constant (\\(a\\)) is the constant.The expectation of a constant multiplied by a random variable is \\(E(aX) = aE(X)\\) When our observations are independent (which we are generally assuming), the variance of the sum is equal to the sum of the variance: \\(Var(\\sum_{i=1}^N X_i) = \\sum_{i=1}^N Var(X_i)\\) When we pull a constant outside the variance operator, we square it: \\(Var(a * X) = a^2Var(X)\\) The steps below for deriving the standard error show why this squaring happens. The variance of a random variable is: \\(Var(X) = E[(X - \\mu)^2]\\) where \\(\\mu\\) refers to the expected value or ``population mean” of the random variable (i.e., \\(E[(X - \\mu)^2] = E[(X - E(X))^2]\\)). Expectation and Variance of a Random Variable The expected value of a random variable (e.g., \\(X\\)) is the average value random variable weighted by its probability of ocurrence. We write it as \\(E(X)\\) or sometimes \\(\\mu_x\\). The variance of a random variable is a measure of spread (written as \\(Var(X)\\) or \\(\\sigma_x^2\\)): the degree to which values of the random variable differ from its expected value. The square root of the variance is the standard deviation, sometimes written as \\(\\sigma_x\\), a measure of spread describing the typical deviation from the expected value. OK: The variance of a random variable is defined as the expected squared deviation from the expected value. Let’s do this for a random variable \\(X_i\\) (i.e., from \\(E(X_i)\\) or \\(\\mu\\)): \\[\\begin{align*} Var(X_i) &amp;= E[(X_i - \\mu)^2] %\\\\ %&amp;= E[X_i^2 - 2 X_i \\mu + \\mu^2]&amp;&amp; \\text{ 1) foil the squared difference}\\\\ %&amp;=E(X_i^2) - 2* E(X_i)*E(X_i) + [E(X_i)]^2 &amp;&amp; \\text{ 2) Move expectation inside, rewrite $\\mu$ as $E%(X_i)$}\\\\ %&amp;= E(X_i^2) - [E(X_i)]^2 &amp;&amp; \\text{ 3) Subtract like terms} \\end{align*}\\] The square root of this quantity is the standard deviation. Expectation and Variance of a Mean We now consider our mean (e.g., \\(\\bar{X}\\)) as our random variable and will derive its variance. Why? Because this gives us the variability in our sampling distribution (how our mean varies) and will get us to our standard error. Recall the standard error is simply the standard deviation of our sampling distribution i.e., the square root of the variance of our sample mean as we think about how the mean varies over all possible randomizations. Recall: The expected value of our sample mean (\\(E(\\bar{X})\\)) can be written as: \\[\\begin{align*} E(\\bar{X}) &amp;= \\frac{1}{N} \\sum_{i=1}^N E(X_i)\\\\ \\end{align*}\\] So we now start with \\(Var(\\bar{X})\\) instead of \\(Var(X_i)\\). However, our variance is still defined as a squared deviation. This time it is the squared deviation of a sample mean from the expected value of the sample mean. \\[\\begin{align*} Var(\\bar{X}) &amp;= E[(\\bar{X} - E(\\bar{X}))^2]\\\\ &amp;= E[(\\frac{1}{N} \\sum_{i=1}^N X_i - E(\\frac{1}{N} \\sum_{i=1}^N X_i))^2] &amp;&amp; \\text{rewrite $\\bar{X}$}\\\\ &amp;= E[(\\frac{1}{N} [\\sum_{i=1}^N X_i - E( \\sum_{i=1}^N X_i)])^2] &amp;&amp; \\text{pull out $\\frac{1}{N}$}\\\\ &amp;= E[\\frac{1}{N^2}(\\sum_{i=1}^N X_i - E( \\sum_{i=1}^N X_i))^2] &amp;&amp; \\text{Note: the squaring of constant}\\\\ &amp;= \\frac{1}{N^2}E[(\\sum_{i=1}^N X_i - E( \\sum_{i=1}^N X_i))^2]&amp;&amp; \\text{Move constant outside expectation}\\\\ &amp;= \\frac{1}{N^2} Var(\\sum_{i=1}^N X_i) &amp;&amp; \\text{Rewrite as variance of the sum of $X_i$}\\\\ &amp;= \\frac{1}{N^2}\\sum_{i=1}^N Var(X_i) &amp;&amp; \\text{Apply rule on variance of sum}\\\\ &amp;= \\frac{1}{N^2}*(Var(X_1) + Var(X_2) +...+ Var(X_N)) &amp;&amp; \\text{Write out the sum}\\\\ &amp;= \\frac{1}{N^2}*N* \\sigma^2 &amp;&amp; \\text{Substitute our known $\\sigma^2$ from above}\\\\ &amp;= \\frac{\\sigma^2}{N} \\end{align*}\\] Note: the square root of this is our standard deviation aka our standard error: \\(\\frac{\\sigma}{\\sqrt{N}}\\). We cannot observe \\(\\sigma\\), so we estimate this using the sample standard deviation \\(s\\). This will be useful, for example, if we want a standard error for the mean outcome in our treatment group. Variance of our Difference in Means Now we last want to quantify the variability in the sampling distribution for our difference in means (\\(\\bar{X}_1 - \\bar{X}_0\\)) assuming our samples and observations are independent. The idea is that every possible randomization similarly generates a different estimate for the difference just as it does for any individual mean. Well we just showed we know the variance of each mean separately: \\(Var(\\bar{X_1}) = \\frac{\\sigma_1^2}{N_1}\\) \\(Var(\\bar{X_0}) = \\frac{\\sigma_0^2}{N_0}\\) Now we have to get the variance of our difference: \\(Var(\\bar{X}_1 - \\bar{X}_0)\\). To do this, we exploit yet another rule for independent samples: that the variance of a difference is equal to the sum of the variances: \\(Var(\\bar{X}_1 - \\bar{X}_0) = Var(\\bar{X}_1) + Var(\\bar{X}_0)\\) The standard deviation is again the square root of this: \\(\\sqrt{Var(\\bar{X}_1) + Var(\\bar{X}_0)}\\) Ok writing this out: \\[\\begin{align*} Var(\\bar{X}_1 - \\bar{X}_0) &amp;= Var(\\bar{X}_1) + Var(\\bar{X}_0)\\\\ &amp;= \\frac{\\sigma_1^2}{N_1} + \\frac{\\sigma_0^2}{N_0} \\end{align*}\\] Now if we want the standard error we get: \\[\\begin{align*} \\sqrt{Var(\\bar{X}_1 - \\bar{X}_0)} &amp;= \\sqrt{Var(\\bar{X}_1) + Var(\\bar{X}_0)}\\\\ &amp;= \\sqrt{\\frac{\\sigma_1^2}{N_1} + \\frac{\\sigma_0^2}{N_0}} \\end{align*}\\] Like before, we use sample substitutes (\\(s_1\\) and \\(s_0\\)) where \\(m\\) represents the number of units in the treatment and \\(N-m\\), the number of unites in the control (switching notation here to match the book): \\[\\begin{align*} \\widehat{SE}_{d-i-m} &amp;= \\sqrt{\\frac{s_1}{m} + \\frac{s_0}{N-m}} \\end{align*}\\] Note: this estimate of the standard error is considered conservative and is only appropriate when samples are independent. We will discuss alternative measures of variance. For example, when we use OLS to get out difference in means, we will use a slightly different “pooled variance estimator” where we assume \\(\\sigma_1^2 = \\sigma_0^2\\). Here, the pooled sample variance is: \\(s_{pooled}^2 = \\frac{(n_1 -1)s^2_1 + (n_0-1)s^2_0}{(n_1 +n_0 -2)}\\) where the standard error is: \\(\\sqrt{s_{pooled}^2} * \\sqrt{(\\frac{1}{n_1}+\\frac{1}{n_0})}\\) "],["equivalence-of-t-test-and-anova.html", "4.7 Equivalence of t-test and ANOVA", " 4.7 Equivalence of t-test and ANOVA You may be familiar with ANOVA as a way to test for group differences. When you have two groups, the t-test and ANOVA are actually equivalent. Let’s convince ourselves of this. ## 1) t-test where variances are assumed to be equal (often we leave this unequal, which adjusts for unequal variances) t1 &lt;- t.test(asians$study2_avg[asians$treatment_cit == 1], asians$study2_avg[asians$treatment_cit == 0], var.equal=T) ## 2) One-way anova ## create variable that is back out vs. stay out only a1 &lt;- aov(study2_avg ~ treatment_cit, data = asians) a1.sum &lt;- summary(a1) ## In one-way ANOVA test, a significant p-value indicates that ## at least one of the group means are different, but we don’t know which ## pairs of groups are different. ## However, if we only have two groups, ANOVA reduces to a t-test ## 3) Also equivalent to simple regression l1 &lt;- lm(study2_avg ~ treatment_cit, data = asians) ## Prove to yourself the equivalence ## 1) Compare p-values of the treatment effect in each case t1$p.value ## [1] 0.02951252 a1.sum[[1]]$`Pr(&gt;F)` ## [1] 0.02951252 NA summary(l1)$coefficients[2, 4] ## [1] 0.02951252 ## 2) Compare test-statistics ## Note it&#39;s the same F-statistic in ANOVA and lm summary(l1)$f ## value numdf dendf ## 4.976264 1.000000 59.000000 a1.sum[[1]]$`F value` ## [1] 4.976264 NA ## And woohoo! the f-statistic is actually equivalent to our t-stat^2 ## (with two groups, sqrt(f) equals the absolute value of t) t1$statistic^2 ## t ## 4.976264 ANOVA and t-test diverge in estimates when you have more than 2 groups ANOVA tests if you have at least one sig. diff (a “joint test” of statistical sig) t-tests are meant for pairwise comparisons of significance BUT there are “post-hoc” anova tests to look at pairwise comparisons, As there are multiple-testing corrections for t-tests More on multiple testing adjustments "],["experimental-design-complications.html", "4.8 Experimental Design Complications", " 4.8 Experimental Design Complications The uncertainty calculations we have done so far have focused on comparisons between two experimental groups, using independent samples (where treatment has been randomly assigned), in a one-shot study. Of course, as we read in section 3, these are not the only experimental designs. We may have designs with more than two experimental groups We may have designs where we measure an outcome both pre-treatment and post-treatment We may have designs where we expose a subject to multiple experimental treatments (e.g., in conjoint studies). Each of these cases may present a slight modifications in how we conduct hypothesis tests and compute uncertainty. We will cover these as we encounter them. "],["visualize.html", "Section 5 Visualization", " Section 5 Visualization In this section, we will walk through some options for visualizing the results of experiments using R. You may wish to refer to the R Graphics cookbook by Winston Chang or Data Visualization by Kieran Healy for additional examples of plotting in R. For considerations on plotting for different types of experimental designs and randomization schemes, you may wish to consult Alex Coppock’s chapter in Advances in Experimental Political Science. This section follows our discussion of survey-based experiments. There are many types of experimental treatments that can be administered via surveys. "],["plotting-average-treatment-effects.html", "5.1 Plotting Average Treatment Effects", " 5.1 Plotting Average Treatment Effects The example we will use is from “Black Politics: How Anger Infuences the Political Actions Blacks Pursue to Reduce Racial Inequality” by Antoine J. Banks, Ismail K. White, and Brian D. McKenzie, published in Political Behavior in 2019. We will replicate the results from Study 2, which is a survey experiment. The sample includes 444 Black treated Black respondents recruited by Qualtrics. The excerpt below shows the experimental manipulation. Here is a short video walking through the code to plot the ATEs using plot and ggplot. (Via youtube, you can speed up the playback to 1.5 or 2x speed.) Let’s load the data. Note: This file is in a .dta format, but if you try to use read.dta to load it, you may receive an error because it is too new of a Stata format. As an alternative, we can use the rio package to open the file. Install the package, open the package with library an load the data. The rio packages uses a single import function to load data. ## install.packages(&quot;rio&quot;, dependencies=T) library(rio) banks &lt;- import(&quot;data/banksstudy2.dta&quot;) The authors have a variable in their data baddata they use to exclude subjects who failed to follow the instructions of their manipulation. They limit their analyses to those who passed this check. Let’s do the same by removing any subjects that have non-missing values on this variable. banks &lt;- subset(banks, is.na(baddata)==T) Let’s replicate a portion of the analysis presented in Table 3 of the paper. We will first calculate our estimate of \\(E(Y_i(1_{anger}) - Y_i(0_{no anger}))\\) using the difference-in-means estimator: \\(\\sum_{i=1}^m Y_i(1_{anger}) - \\sum_{m+1}^{N-m}Y_i(0_{no anger})\\). We will compare those in the Anger and Control conditions on the outcome for donations to Black organizations. We will use a t-test to do so. d.i.m &lt;- mean(banks$blackdon[banks$angvcon == 1], na.rm=T) - mean(banks$blackdon[banks$angvcon == 0], na.rm=T) t.results &lt;- t.test(banks$blackdon[banks$angvcon == 1], banks$blackdon[banks$angvcon == 0]) ci &lt;- t.results$conf.int Let’s repeat for the hope condition. d.i.m2 &lt;- mean(banks$blackdon[banks$hopevcon == 1], na.rm=T) - mean(banks$blackdon[banks$hopevcon == 0], na.rm=T) t.results2 &lt;- t.test(banks$blackdon[banks$hopevcon == 1], banks$blackdon[banks$hopevcon == 0]) ci2 &lt;- t.results2$conf.int We could have alternatively used a linear regression to assess significance or randomization inference. Expand for a randomization inference example. Let’s focus on just the Anger vs. Control first. angercontrol &lt;- subset(banks, angvcon == 0 | angvcon ==1) ## remove missing data angercontrol &lt;- subset(angercontrol, is.na(blackdon) ==F) ## install.packages(&quot;ri2&quot;, dependencies=T) library(ri2) ## Declare randomization declaration &lt;- declare_ra(N=nrow(angercontrol), prob=.5) ## Estimate the average treatment effect set.seed(1215) ri2_out &lt;- conduct_ri( formula = blackdon ~ angvcon, assignment = &quot;angvcon&quot;, declaration = declaration, sharp_hypothesis = 0, data = angercontrol ) Plot and compare distribution to observed ATE plot(ri2_out) Summarize Output summary(ri2_out) ## term estimate two_tailed_p_value ## 1 angvcon 0.9793778 0.041 We can manually see what the package is doing by counting how many of the simulated estimates from the empirical distribution of the sharp null hypothesis were more extreme than our estimate from the study. Note that in this case, our p-value is very similar to the t-test. estimate &lt;- tidy(ri2_out)$estimate nsims &lt;- length(ri2_out$sims_df$est_sim) simstimates &lt;- ri2_out$sims_df$est_sim ## Two-tailed p-value length(simstimates[abs(simstimates) &gt;= abs(estimate)])/nsims ## [1] 0.041 We can compare this to the p-value through the t-test where we assume a t distribution and calculate the area at the extremes as larger or larger than our t-statistic. 5.1.1 ATE using plot When we want to visualize results in R, generally we plot the main Quantity of Interest Usually the estimated average treatment effect and/or average outcome from each condition condition With uncertainty estimates Potentially also showing the distribution of underlying data Some marker to show a relative benchmark (e.g., a line at 0) The plot function in R is based on a coordinate system. We supply the x= and y= values where we want to place points. We will make a plot to display the two ATE estimates we just calculated. We need to supply the exact same number of values for the x-axis as the y-axis. Let’s plot the ATE estimates at points 1 and 2 on the x-axis and at the corresponding y-values for the ATEs we estimated. ## Plot plot(x = c(1, 2), y = c(d.i.m, d.i.m2)) This has created the plot, but it is not very informative. Let’ set the axis dimensions with xlim= an ylim= Let’s add a title with main= We can adjust the size of the title text with cex.main Let’s add a label for y and x axis with ylab and xlab We can adjust the size of the labels cex.lab plot(x = c(1, 2), y = c(d.i.m, d.i.m2), xlim=c(.5, 2.5), ylim = c(-1, 2), main=&quot;Average Treatment Effects on Donations to Black Organizations&quot;, cex.main=.8, ylab=&quot;Difference in Donation Amount&quot;, xlab= &quot;Treatment Comparison&quot;, cex.lab=.8) In our case, the values on the x-axis are meaningless. We arbitrarily placed the points at 1 and 2. Let’s get rid of the current x-axis and instead replace it with an axis that labels our comparisons. We get rid of the current x-axis with xaxt=\"n\" We create a new axis using the axis function. Note: this function goes below the plot() function instead of inside it. plot(x = c(1, 2), y = c(d.i.m, d.i.m2), xlim=c(.5, 2.5), ylim = c(-1, 2), main=&quot;Average Treatment Effects on Donations to Black Organizations&quot;, cex.main=.8, ylab=&quot;Difference in Donation Amount&quot;, xlab= &quot;Treatment Comparison&quot;, cex.lab=.8, xaxt=&quot;n&quot;) axis(1, at=1:2, labels=c(&quot;Anger vs. \\n Control&quot;,&quot;Hope vs. \\n Control&quot;), tick=F) We now have an informative plot of our ATE quantities of interest. However, we still need to add something to visualize uncertainty and a benchmark to indicate the size and/or significance of our quantities. We can add a horizonatal line to the plot with abline(h=). Like axis(), this function goes below the plot() function. We can add confidence intervals as vertical line segments to our plot using the lines function. Again, this adds a layer below our plot. plot(x = c(1, 2), y = c(d.i.m, d.i.m2), xlim=c(.5, 2.5), ylim = c(-1, 2), main=&quot;Average Treatment Effects on Donations to Black Organizations&quot;, cex.main=.8, ylab=&quot;Difference in Donation Amount&quot;, xlab= &quot;Treatment Comparison&quot;, cex.lab=.8, xaxt=&quot;n&quot;) axis(1, at=1:2, labels=c(&quot;Anger vs. \\n Control&quot;,&quot;Hope vs. \\n Control&quot;), tick=F) abline(h=0, col=&quot;red3&quot;, lty=2) lines(c(1,1), ci) lines(c(2,2), ci2) 5.1.2 ATE with ggplot The package ggplot2 also offers a system of plotting in R. The “gg” in ggplot2 stands for the “Grammar of Graphics.” This program provides another framework for creating figures in R. According to Hadley Wickham, “ggplot2 provides beautiful, hassle-free plots that take care of fiddly details like drawing legends.” Practically speaking, ggplot() is another tool to plot the same types of figures we have been making in class. Some people prefer ggplot2 because they find the logic of building figures more intuitive using this framework and/or more aesthetically pleasing. However, both ggplot() and the plots we have been making in class can accomplish the same ultimate goals of data visualization– to communicate information transparently, quickly, accurately, simply, and beautifully. Which types of plots you may prefer is up to your own taste. The syntax for this is different. One of the primary differences is that the ggplot function generally requires that you start from a data.frame object. This means that we will have to organize the set of results we want to plot into a rectangular data.frame. ## Put each result in a vector angerresults &lt;- c(d.i.m, ci) hoperesults &lt;- c(d.i.m2, ci2) ## Bind these together as rows, store as dataframe comb &lt;- data.frame(rbind(angerresults, hoperesults)) ## Give columns informative labels names(comb) &lt;- c(&quot;ATE&quot;, &quot;lower&quot;, &quot;upper&quot;) ## Add group indicator comb$Comparison &lt;- c(&quot;Anger vs. \\n Control&quot;,&quot;Hope vs. \\n Control&quot;) Now we can use the ggplot function from the ggplot2 package. The main plotting function in ggplot2 is the ggplot() function. It will give you access to barplots, boxplots, scatterplots, histograms, etc. The three primary components of a ggplot() are a dataframe (data =), a set of mapping aesthetics (aes()), and geoms (e.g., geom boxplot, geom bar, geom point, geom line, etc.). The function ggplot() first takes a dataframe that includes the values you would like to plot (e.g., data = comb). The aesthetics then include the variable names that you want to plot on the x and y axis (e.g., aes(x=Comparison, y=ATE)) Additional mapping aesthetics can be specified. For example, a third variable (or a repeat of a previous variable) can also be specified (e.g., fill =, colour =, shape =), which acts as a grouping variable. If this is specified, ggplot() will create a corresponding legend for the plot and will color/make different shapes for different groups within this third variable. After closing out the first ggplot() parentheses, you then annotate the plot by adding (+) a geometric layer. In the example below, we use the geom_point layer to add the ATEs and geom_errorbar layer to add confidence intervals. There are many more possibilities for plotting with ggplot(). For additional resources on all that is gg, I recommend the R Graphics Cookbook. library(ggplot2) ggplot(comb, aes(x=Comparison, y=ATE))+ geom_point()+ geom_errorbar(aes(ymin=lower, ymax=upper), width=.1)+ theme_bw() Just like with the other plotting functions in R, you can also specify a number of other arguments to make your plot more informative and aesthetically pleasing. Here, you do this by adding (+) additional arguments. See examples below (e.g., ggtitle, xlab, ylab for titles, ylim for y-axis limits, etc.). We can also add a horizontal line with geom_hline. ggplot(comb, aes(x=Comparison, y=ATE))+ geom_point()+ geom_errorbar(aes(ymin=lower, ymax=upper), width=.1)+ theme_bw()+ geom_hline(aes(yintercept=0), linetype=&quot;dashed&quot;, colour=&quot;red3&quot;)+ ggtitle(&quot;Average Treatment Effects on Donations to Black Organizations&quot;)+ theme(plot.title = element_text(hjust = 0.5))# centers title "],["heterogeneous-treatment-effects-1.html", "5.2 Heterogeneous Treatment Effects", " 5.2 Heterogeneous Treatment Effects Here is a short video walking through the code to plot the ATEs using plot and ggplot. (Via youtube, you can speed up the playback to 1.5 or 2x speed.) Let’s replicate Figure 4 of the paper to study heterogeneous treatment effects. The authors compute these using a regression analysis. We will focus on the Anger vs. Control condition. First, let’s limit the sample to just these two conditions. angcontrol &lt;- subset(banks, angvcon == 1 | angvcon == 0) We will look at how the effect of anger varies across the Community Nationalism Scale in the variable blackauto3. This is a three-point scale with points at 0,1, and 2. We could treat this as a numeric variable or as a categorical variable. We will first do it as a categorical variable. ## option 1- categorical fit &lt;- lm(blackdon ~ angvcon*factor(blackauto3), data=angcontrol) summary(fit) ## ## Call: ## lm(formula = blackdon ~ angvcon * factor(blackauto3), data = angcontrol) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.8611 -3.1750 -0.1909 3.3953 7.1579 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.8421 0.6330 4.490 1.07e-05 *** ## angvcon 0.3329 0.8840 0.377 0.707 ## factor(blackauto3)1 1.0051 0.9076 1.107 0.269 ## factor(blackauto3)2 1.3488 0.8231 1.639 0.103 ## angvcon:factor(blackauto3)1 0.4245 1.2484 0.340 0.734 ## angvcon:factor(blackauto3)2 1.3373 1.1577 1.155 0.249 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.902 on 260 degrees of freedom ## (8 observations deleted due to missingness) ## Multiple R-squared: 0.06442, Adjusted R-squared: 0.04643 ## F-statistic: 3.581 on 5 and 260 DF, p-value: 0.003779 Focus on the interaction term when interpreting the results for the heterogeneous treatment effects. ## option 2- numeric fit.numeric &lt;- lm(blackdon ~ angvcon*blackauto3, data=angcontrol) We can then calculate the average treatment effects within each level of blackauto3 using the margins() function in R from the margins package. The first input is the object name for the regression model (e.g., fit). The next input is a list of variables and their corresponding values for which you want to hold constant while estimating marginal effects of some other variable The variable input is then the treatment condition, or the variable for which you want to estimate the marginal effect on the outcome. The change then describes the two values of the treatment condition variable that represent the control vs. treatment. In this case it was 0 vs. 1. If your treatment condition is a factor variable, you probably don’t need to specify this. library(margins) outp &lt;- margins(fit, at = list(blackauto3 = c(0, 1, 2)), variable = &quot;angvcon&quot;, change = c(0, 1)) summary(outp) ## factor blackauto3 AME SE z p lower upper ## angvcon 0.0000 0.3329 0.8840 0.3766 0.7065 -1.3996 2.0654 ## angvcon 1.0000 0.7574 0.8815 0.8592 0.3902 -0.9703 2.4852 ## angvcon 2.0000 1.6702 0.7475 2.2342 0.0255 0.2050 3.1354 The summary is already in a nice dataframe format, which makes it easy to use ggplot. outp.df &lt;- summary(outp) ggplot(outp.df, aes(x=blackauto3, y=AME))+ geom_point()+ geom_errorbar(aes(ymin=lower, ymax=upper), width=.1)+ theme_bw()+ geom_hline(aes(yintercept=0), linetype=&quot;dashed&quot;, colour=&quot;red3&quot;)+ ggtitle(&quot;Average Treatment Effects on Donations to Black Organizations \\n by Community nationalism&quot;)+ ylab(&quot;Average Treatment Effects on Black Org. Donations&quot;)+ xlab(&quot;Community Nationalism&quot;)+ scale_x_continuous(breaks = c(0, 1, 2), labels = c(&quot;Low&quot;, &quot;Medium&quot;, &quot;High&quot;))+ theme(plot.title = element_text(hjust = 0.5))# centers title Here’s an alternative way to look at it with geom_line and geom_ribbon ggplot(outp.df, aes(x=blackauto3, y=AME))+ geom_point()+ geom_line()+ geom_ribbon(aes(ymin=lower, ymax=upper), alpha=.4)+ theme_bw()+ geom_hline(aes(yintercept=0), linetype=&quot;dashed&quot;, colour=&quot;red3&quot;)+ ggtitle(&quot;Average Treatment Effects on Donations to Black Organizations \\n by Community nationalism&quot;)+ ylab(&quot;Average Treatment Effects on Black Org. Donations&quot;)+ xlab(&quot;Community Nationalism&quot;)+ scale_x_continuous(breaks = c(0, 1, 2), labels = c(&quot;Low&quot;, &quot;Medium&quot;, &quot;High&quot;))+ theme(plot.title = element_text(hjust = 0.5))# centers title An alternative way to represent heterogeneity is instead of plotting the conditional average treatment effects, we can plot the raw outcomes in each condition. We can then calculate the estimated outcomes using the prediction package in R. It works somewhat similarly to margins except we do not specify the variable for estimating the marginal effect. Instead, we fold the treatment variable into the list argument. library(prediction) outp2 &lt;- prediction(fit, at = list(blackauto3 = c(0, 1, 2), angvcon = c(0,1)), calculate_se = T) summary(outp2) ## at(blackauto3) at(angvcon) Prediction SE z p lower upper ## 0 0 2.842 0.6330 4.490 7.129e-06 1.601 4.083 ## 1 0 3.847 0.6504 5.916 3.308e-09 2.573 5.122 ## 2 0 4.191 0.5262 7.965 1.652e-15 3.160 5.222 ## 0 1 3.175 0.6170 5.146 2.661e-07 1.966 4.384 ## 1 1 4.605 0.5951 7.738 1.010e-14 3.438 5.771 ## 2 1 5.861 0.5310 11.038 2.518e-28 4.820 6.902 outp2.df &lt;- data.frame(summary(outp2)) ggplot(outp2.df, aes(x=at.blackauto3., y=Prediction, fill=as.factor(at.angvcon.)))+ geom_point()+ geom_line()+ geom_ribbon(aes(ymin=lower, ymax=upper), alpha=.4)+ theme_bw()+ ggtitle(&quot;Average Donations to Black Organizations \\n by Community nationalism&quot;)+ ylab(&quot;Average Black Org. Donations&quot;)+ xlab(&quot;Community Nationalism&quot;)+ scale_fill_manual(&quot;Condition&quot;, labels=c(&quot;Control&quot;, &quot;Anger&quot;), values=c(&quot;orange&quot;, &quot;dodgerblue&quot;))+ scale_x_continuous(breaks = c(0, 1, 2), labels = c(&quot;Low&quot;, &quot;Medium&quot;, &quot;High&quot;))+ theme(plot.title = element_text(hjust = 0.5))# centers title "],["some-additional-plotting-options.html", "5.3 Some additional plotting options", " 5.3 Some additional plotting options The common visualizations used to show average treatment effects do not give much information about the distributions of underlying data. Here are a few examples of plotting the underlying distributions. You might also explore geom_bar for outcomes that are binary or categorical in nature as an alternative to geom_histogram. Create a variable that summarizes all three experimental conditions. This will make it easier to plot data grouped by each condition. banks$condition &lt;- NA banks$condition[banks$angvcon == 1] &lt;- &quot;Anger&quot; banks$condition[banks$hopevcon == 1] &lt;- &quot;Hope&quot; banks$condition[banks$angvcon == 0 &amp; banks$hopevcon == 0] &lt;- &quot;Control&quot; banks$condition &lt;- factor(banks$condition, levels=c(&quot;Control&quot;, &quot;Anger&quot;, &quot;Hope&quot;)) We filter out respondents who were not assigned to any condition. You can do this as part of the plot code, or you can banks &lt;- subset(banks, is.na(condition)==F) prior to running the plot code. library(tidyverse) banks %&gt;% filter(is.na(condition)==F) %&gt;% ggplot(aes(x=blackdon, fill=condition))+ geom_histogram(alpha=.4)+ theme_bw()+ ggtitle(&quot;Distribution of Donations to Black Organizations&quot;)+ theme(plot.title = element_text(hjust = 0.5), legend.position = &quot;bottom&quot;) + facet_grid(~condition)+ xlab(&quot;Amount Donation (dollars)&quot;) banks %&gt;% filter(is.na(condition)==F) %&gt;% ggplot(aes(y=blackdon, x=condition, color=condition))+ geom_boxplot()+ geom_jitter(alpha=.5)+ theme_bw()+ ggtitle(&quot;Distribution of Donations to Black Organizations&quot;)+ theme(plot.title = element_text(hjust = 0.5), legend.position = &quot;bottom&quot;) + ylab(&quot;Amount Donation (dollars)&quot;) banks %&gt;% filter(is.na(condition)==F) %&gt;% ggplot(aes(x=blackdon, fill=condition))+ geom_density(alpha=.5)+ theme_bw()+ ggtitle(&quot;Distribution of Donations to Black Organizations&quot;)+ theme(plot.title = element_text(hjust = 0.5)) + facet_grid(~condition)+ xlab(&quot;Amount Donation (dollars)&quot;) Here is a way to show the means and confidence intervals for each condition. This figure is based on the plot in Figure 17.1 from Alex Coppock’s chapter in Advances in Experimental Political Science. ## Find means and confidence intervals by condition banks &lt;- subset(banks, is.na(condition)==F) m.cond &lt;- tapply(banks$blackdon, banks$condition, mean, na.rm=T) ci.hope &lt;- t.test(banks$blackdon[banks$condition == &quot;Hope&quot;])$conf.int ci.anger &lt;- t.test(banks$blackdon[banks$condition == &quot;Anger&quot;])$conf.int ci.control &lt;- t.test(banks$blackdon[banks$condition == &quot;Control&quot;])$conf.int combd &lt;- data.frame(cbind(cbind(m.cond),rbind(ci.control, ci.anger, ci.hope))) names(combd) &lt;- c(&quot;Mean&quot;, &quot;lower&quot;, &quot;upper&quot;) combd$condition &lt;- c(&quot;Control&quot;, &quot;Anger&quot;, &quot;Hope&quot;) combd$condition &lt;- factor(combd$condition, levels=c(&quot;Control&quot;, &quot;Anger&quot;, &quot;Hope&quot;)) ## Note we draw from both data=combd and data=banks ggplot(combd, aes(x=condition, y=Mean, color=condition)) + geom_point(data = banks, aes(y=blackdon), position = position_jitter(width = 0.2, height = 0.1), alpha = 0.4) + geom_point(size = 3) + geom_errorbar(aes(ymin = lower, ymax = upper), width = 0) + theme_bw() + ggtitle(&quot;Donations to Black Organizations by Condition&quot;)+ theme(plot.title = element_text(hjust = 0.5)) + scale_y_continuous(breaks = seq(0, 10, length.out = 5)) + theme(axis.title.x = element_blank()) + ylab(&quot;Donations (dollars)&quot;) "],["examples-of-arguments-in-plot.html", "5.4 Examples of arguments in plot", " 5.4 Examples of arguments in plot Here are some common R plotting functions and arguments Create a plot plot(): for scatterplots and trend plots barplot(): for barplot comparisons across categories boxplot(): boxplot for summaries of numeric variables hist(): for histogram summaries of a single numeric variable Aesthetic arguments within a plot main =: Specifies the main title of the plot. Supply text (e.g., main = \"my title\") ylab =: Specifies the title of the y-axis. Supply text (e.g., ylab = \"Mean of variable\") xlab =: Specifies the title of the x-axis. Supply text (e.g., xlab = \"X variable name\") ylim =: Specifies the range of the y-axis. Supply vector of two numbers (e.g., ylim = c(0, 100)) xlim =: Specifies the range of the x-axis. Supply vector of two numbers (e.g., xlim = c(0, 100)) bty=\"n\": Removes the border box around the plot cex, cex.main, cex.names, cex.lab, cex.axis: Changes the size of different elements of a plot. Default is 1, so a value of .8 would be smaller than default, and 1.2 would be bigger than normal. type =: Specifies the type of plot (e.g., type=\"l\" is a line plot, type=\"b\" is a plot with points and lines connecting them) lwd=: Specifies the width of a line on a plot. Default is 1. E.g., lwd=3 makes a line much thicker pch=: Specifies the point type. E.g., pch=15 lty=: Specifies the line type. E.g., lty=2 is a dashed line col=: Specifies the color of the central element of the plot. Can take a single color or vector of colors. Use colors() in the console to see all R colors. names: Specifies a set of labels in a barplot Ways to annotate a plot (generally added below the initial plotting function) abline(): Adds a line to the plot at a particular point on the x- or y- intercept, either horizontal, vertical, or of a particular slope Example: Adding a horizontal line at a particular at a y value of 2 abline(h=2) Example: Adding a vertical line at a particular at a x value of 2 abline(v=2) lines(x=, y=): Adds a line connecting pairs of x- and y-coordinates. We used this to add the South line to the social mobility plot. axis(): Used to replace the default x- or y- axis that R will create with a customized axis To create an original y-axis, use axis(2, vectorofvalues, labels) and specify yaxt=\"n\" inside the plotting function to remove the original y-axis. To create an original x-axis, use axis(1, vectorofvalues, labels) and specify xaxt=\"n\" inside the plotting function to remove the original x-axis. legend(): Adds a legend to a plot. Can specify the location as the first argument (e.g., \"bottomleft\" or \"topright\") text(): Adds text to a plot at specific x- and y- locations. (E.g., text(x=3, y=4, \"Here is a point\"). The x and y arguments can be single numbers or a vector of numbers. x and y need to be the same length. points(): Adds points to a plot at specific x- and y- locations. Inputs are much like plot "],["creating-tables-from-r.html", "5.5 Creating Tables from R", " 5.5 Creating Tables from R Formatting and Exporting R Results R has a number of tools, including the packages texreg, xtable, and stargazer, which can be used to export tables made in R to nicely formatted LaTex or html output. Here is a link to the texreg package documentation. Section 5 has examples of the texreg and htmlreg functions within the texreg package. These can be integrated into R Markdown and Sweave documents, and their output can be pasted into LaTex or Microsoft Word. Your choice of function will depend on where you ultimately want your results to be compiled. If you are generating results that will be compiled to pdf using LaTex, then texreg works well. If you are exporting results to Word, than you may wish to use the htmlreg function within the texreg package, which will generate output that can be pasted into Word. A simple example using R Markdown html output. (Note, if you wanted to export the table to Word, you would add an argument specifying file = \"myfit.doc\" to the function. See the above link for examples: mydata &lt;- read.csv(&quot;https://raw.githubusercontent.com/ktmccabe/teachingdata/main/resume.csv&quot;) fit &lt;- lm(call ~ race, data=mydata) ## First time you use texreg, install it install.packages(&quot;texreg&quot;) library(texreg) htmlreg(list(fit), stars=c(0.001, 0.01, 0.05), caption = &quot;Regression of Call Backs on Race&quot;) Regression of Call Backs on Race Model 1 (Intercept) 0.06*** (0.01) racewhite 0.03*** (0.01) R2 0.00 Adj. R2 0.00 Num. obs. 4870 p &lt; 0.001; p &lt; 0.01; p &lt; 0.05 You can add more arguments to the function to customize the name of the model and the coefficients. You can also add multiple models inside the list argument, for example, if you wanted to present a table with five regression models at once. Here is an example with two: fit2 &lt;- lm(call ~ race + sex, data=mydata) library(texreg) htmlreg(list(fit, fit2), stars=c(0.001, 0.01, 0.05), caption = &quot;Regression of Call Backs on Race and Sex&quot;) Regression of Call Backs on Race and Sex Model 1 Model 2 (Intercept) 0.06*** 0.07*** (0.01) (0.01) racewhite 0.03*** 0.03*** (0.01) (0.01) sexmale -0.01 (0.01) R2 0.00 0.00 Adj. R2 0.00 0.00 Num. obs. 4870 4870 p &lt; 0.001; p &lt; 0.01; p &lt; 0.05 5.5.1 Additional formatting examples Here are some additional examples with different formats. You can run them on your own computer to see what the output looks like. The package texreg has three primary formats texreg() for LATEX output; htmlreg() for HTML, Markdown-compatible and Microsoft Word-compatible output; screenreg() for text output to the R console. If you are working with a LaTex document, I recommend using texreg(), which will output LaTex syntax in your R console, which you can copy and paste into your article document. Note: this function allows you to customize model and coefficient names. library(texreg) texreg(list(fit, fit2), stars=c(0.001, 0.01, 0.05), caption = &quot;Regression of Call Backs on Race and Sex&quot;, custom.model.names = c(&quot;Bivariate&quot;, &quot;Includes Sex&quot;), custom.coef.names = c(&quot;Intercept&quot;, &quot;Race- White&quot;, &quot;Sex- Male&quot;)) If you are working with a Microsoft Word document, I recommend using htmlreg() and specifying a file name for your output. This will export a file to your working directory, which you can copy and paste into your Word article document. Otherwise, the syntax is the same as above. library(texreg) htmlreg(list(fit, fit2), file = &quot;models.doc&quot;, stars=c(0.001, 0.01, 0.05), caption = &quot;Regression of Call Backs on Race and Sex&quot;, custom.model.names = c(&quot;Bivariate&quot;, &quot;Includes Sex&quot;), custom.coef.names = c(&quot;Intercept&quot;, &quot;Race- White&quot;, &quot;Sex- Male&quot;)) If you are trying to read the output in your R console, that’s when I would use screenreg(). However, for professional manuscript submissions, I would recommend the other formats. library(texreg) screenreg(list(fit, fit2), stars=c(0.001, 0.01, 0.05), caption = &quot;Regression of Call Backs on Race and Sex&quot;, custom.model.names = c(&quot;Bivariate&quot;, &quot;Includes Sex&quot;), custom.coef.names = c(&quot;Intercept&quot;, &quot;Race- White&quot;, &quot;Sex- Male&quot;)) The package stargazer allows similar options. I don’t think there are particular advantages to either package. Whatever comes easiest to you. The default for stargazer will output LaTex code into your R console. Note that the syntax is similar but has slightly different argument names from the texreg package. Also, the intercept is at the bottom by default for stargazer. Be careful of the covariate ordering when you add labels. library(stargazer) stargazer(list(fit, fit2), star.cutoffs=c(0.05,0.01, 0.001), title= &quot;Regression of Call Backs on Race and Sex&quot;, dep.var.labels.include = F, column.labels = c(&quot;Call Back&quot;, &quot;Call Back&quot;), covariate.labels = c(&quot;Race- White&quot;, &quot;Sex- Male&quot;, &quot;Intercept&quot;)) You can adjust the type of output in stargazer for other formats, similar to texreg. Here is an example of Microsoft Word output. library(stargazer) stargazer(list(fit, fit2), out = &quot;modelstar.doc&quot;, type=&quot;html&quot;, star.cutoffs=c(0.05,0.01, 0.001), dep.var.labels.include = F, title= &quot;Regression of Call Backs on Race and Sex&quot;, column.labels = c(&quot;Call Back&quot;, &quot;Call Back&quot;), covariate.labels = c(&quot;Race- White&quot;, &quot;Sex- Male&quot;, &quot;Intercept&quot;)) 5.5.2 Additional Table Types Sometimes you might want to create tables that are not from regression models, such as tables for descriptive statistics. R has other packages for tables of this type. For example xtable can create simple html and latex tables. You just have to supply the function with a table object or matrix. Here is a first example making a formated table using crosstabs of two variables. mydata &lt;- read.csv(&quot;https://raw.githubusercontent.com/ktmccabe/teachingdata/main/resume.csv&quot;) library(xtable) table1 &lt;- table(race = mydata$race, sex = mydata$sex) ## RMarkdown html print(xtable(table1), type=&quot;html&quot;) female male black 1886 549 white 1860 575 ## LaTeX print(xtable(table1)) ## Word print(xtable(table1), type=&quot;html&quot;, file = &quot;crosstab.doc&quot;) Example with a t-test We assemble the results in a vector. Using rbind makes it into a matrix object. If you had multiple t-tests, you could rbind() several vector rows together into one xtable I extracted the estimates, t-statistic, and confidence intervals. You could also extract the p-value. ## Run t-test and gather results mydata &lt;- read.csv(&quot;https://raw.githubusercontent.com/ktmccabe/teachingdata/main/resume.csv&quot;) t.resume &lt;- t.test(mydata$call[mydata$race == &quot;black&quot;], mydata$call[mydata$race == &quot;white&quot;]) estimates &lt;- t.resume$estimate cinterval &lt;- t.resume$conf.int tstat &lt;- t.resume$statistic results &lt;- c(estimates, cinterval, tstat) names(results) &lt;- c(&quot;Mean Black App&quot;, &quot;Mean White App&quot;, &quot;Lower CI&quot;, &quot;Upper CI&quot;, &quot;t-statistic&quot;) results &lt;- rbind(results) library(xtable) ## Rmarkdown html print(xtable(results), type=&quot;html&quot;, include.rownames = F) Mean Black App Mean White App Lower CI Upper CI t-statistic 0.06 0.10 -0.05 -0.02 -4.11 library(xtable) ## Word doc print(xtable(results), type=&quot;html&quot;, include.rownames = F, file=&quot;tresults.doc&quot;) library(xtable) ## Latex print(xtable(results), include.rownames = F) "],["ethics.html", "Section 6 Ethics and Sampling Considerations", " Section 6 Ethics and Sampling Considerations In this section, we touch on different ethical issues that can arise when conducting experiments, as well as considerations we should have when recruiting subjects and assessing data quality. We touch on the following resources: Teele, D. (2021). Virtual Consent: The Bronze Standard for Experimental Ethics. In J. Druckman &amp; D. Green (Eds.), Advances in Experimental Political Science (pp. 130-146). Cambridge: Cambridge University Press. Konnikova, Maria. 2015. “How a Gay-Marriage Study Went Wrong.” The New Yorker. Johnson, Jeremy. 2015. “Campaign Experiment found to be in Violation of Montana Law.” Washington Post. In Defense of the Montana Experiment by Thomas Leeper. Boudreau, Cheryl. (2021). Transparency in Experimental Research. In J. Druckman &amp; D. Green (Eds.), Advances in Experimental Political Science (pp. 339-353). Cambridge: Cambridge University Press. doi:10.1017/9781108777919.024 Krupnikov, Yanna, Nam, Hannah, &amp; Style, Hillary. (2021). Convenience Samples in Political Science Experiments. In J. Druckman &amp; D. Green (Eds.), Advances in Experimental Political Science (pp. 165-183). Cambridge: Cambridge University Press. doi:10.1017/9781108777919.012 Mummolo, Jonathan and Erik Peterson. “Demand Effects in Survey Experiments: An Empirical Assessment.” APSR https://doi.org/10.1017/S0003055418000837 "],["value-of-informed-consent.html", "6.1 Value of Informed Consent", " 6.1 Value of Informed Consent Below we will use some of these questions to guide our discussion. p.comment { background-color: #DBDBDB; padding: 10px; border: 1px solid black; margin-left: 25px; border-radius: 5px; font-style: italic; } As Teele reviews, the Belmont report covers three principles. How should we define these?: Beneficence Respect for persons Justice Here is a summary of different forms of consent: Which one is the most common for the experiments we have designed? What is the value of informed consent? What does it try to achieve Your ideas … When might informed consent undermine research goals? Can it ever actually increase harm to subjects? Your ideas … How can we resolve tradeoffs between informed consent and measurement? Should we? Must we? Your ideas … In cases where we do not get informed consent for seemingly valuable reasons, can this go awry? Are there potential downstream consequences? Does it depend on the sample size? Or study design? Is the research still worth it in the end? Your ideas … See example mentioned in response to a study varying the names used in emails to Colorado county clerks: “My name is Karim and I hope you are well. I found your contact information in a voting resources directory and I want to ask about the voting process. What do I need to bring to vote? I want to vote for president but I did not register with a political party. Do I have to do that before I vote. And if I have to work late will I still be able to vote in time.” See example of mailers sent to more than 100,000 Montana registered voters for a nonpartisan judicial election. "],["research-integrity-reproducibility-and-transparency.html", "6.2 Research Integrity, Reproducibility, and Transparency", " 6.2 Research Integrity, Reproducibility, and Transparency p.comment { background-color: #DBDBDB; padding: 10px; border: 1px solid black; margin-left: 25px; border-radius: 5px; font-style: italic; } How do the incentives that structure academia encourage vs. discourage research fraud? Your ideas … Are there steps the field has taken / can take to detect and mitigate it? Your ideas … When our findings don’t replicate, how should we interpret this? Does it mean the original result was a false positive? Your ideas … p.comment { background-color: #DBDBDB; padding: 10px; border: 1px solid black; margin-left: 25px; border-radius: 5px; font-style: italic; } 6.2.1 Preregistration What is pre-registration? (from Boudreau) “Practice of developing one’s research questions, hypotheses, research design, and analyses before observing the data and making that information public on an independent registry.” “Researchers may also create and submit pre-analysis plans that describe in detail the procedures they will use when collecting and analyzing the data (e.g., planned data analyses and statistical tests).” These can also include standard operating procedures Here is a short guide to a pre-analysis plan from EGAP Examples of pre-registration registries: Aspredicted.org; Open Science Framework; EGAP (now hosted by OSF) Note that some journals now require pre-registration for experiments. E.g., The Journal of Politics Some journals now offer a chance to submit a registered report, where your paper is reviewed blind to the results. E.g., Journal of Experimental Political Science. See a discussion from the editor Vin Arceneaux here. p.comment { background-color: #DBDBDB; padding: 10px; border: 1px solid black; margin-left: 25px; border-radius: 5px; font-style: italic; } What items should be included in a pre-registration plan? Your ideas … What are the benefits of pre-registration? Are there downsides? Your ideas … 6.2.2 Reporting an Experimental Analysis (from Boudreau and Gerber et al. 2015) Eligibility and exclusion criteria for participants Details of recruitment and selection of participants, including incentives and any firms used Type of experiment (lab, survey, field), mode, location, and dates conducted Response rate or other participation metric (and how calculated), when possible Details of randomization procedure Baseline means and standard deviations for demographics and other pretreatment measures by experimental group Whether blinding took place and how it was accomplished Description of the treatment(s), as well as description of the control group Details of experiment: its duration, number of participants, within- versus between-subject design, piggybacking/ordering/repetition of treatments, use of deception, use of incentives Evidence treatment was delivered as intended, if available Definitions of outcome measures and covariates, as well as noting whether the level of analysis differs from the level of randomization Identification of analyses specified ex ante versus ex post exploratory analyses Information in CONSORT participant flow diagram Sample means and standard deviations for outcome variables using intent-to-treat analysis Patterns of missing data, attrition, and methods of addressing these issues if missing data and/or attrition are present Description of weighting procedures, if used Institutional review board approval, preregistration, source of funding, conflicts of interest Availability of replication materials and data set Many researchers share data via Dataverse, OSF, or Github "],["sampling-considerations.html", "6.3 Sampling Considerations", " 6.3 Sampling Considerations p.comment { background-color: #DBDBDB; padding: 10px; border: 1px solid black; margin-left: 25px; border-radius: 5px; font-style: italic; } What makes a sample a good sample? Your ideas … Who is in our sample? For an average person’s discussion of polling and sampling and participating in surveys, see 37-39:40 minutes of the Nateland podcast. How can we check for data quality? What elements are a part of data quality? I.e., what should we be worried about? Your ideas … What are examples of bot checks/attention checks? Your ideas … When should we actually exclude subjects? When should we not? Your ideas … 6.3.1 Power Analysis We are often concerned about guarding against false positives. We do this by setting a conservative threshold for judging significance in hypothesis testing. Type I error: “false positive”: the error of rejecting a null hypothesis when it is actually true +Conventionally, our tolerance for false positives are \\(\\alpha = 0.05\\). Type II error: “false negative”: conclude there is no effect (failing to reject the null) when there is one. We tend to refer to this as \\(\\beta\\) and statistical power is \\(1-\\beta\\) (true positive) What is a test’s Power? Power helps us guard against false negatives. It is the probability of a true positive: Finding a significant effect if one is there, (1- Type II) where a Type II error is when you conclude there is no effect when there is one. See discussion on power from EGAP. \\[\\begin{align*} 1 - Pr(\\text{Type II error}) &amp;= 1 - \\beta\\\\ &amp;= \\underbrace{\\Phi (\\frac{| \\mu_t -\\mu_c|\\sqrt{N}}{2\\sigma} - \\Phi^{-1}(1 - \\frac{\\alpha}{2}))}_{\\text{A common formula}} \\\\ &amp;= \\Phi (\\frac{| \\mu_t -\\mu_c|\\sqrt{N}}{2\\sigma} - \\underbrace{ 1.96}_{\\text{At conventional levels}})\\\\ &amp;= \\text{Prob test stat exceeds threshold for rejecting null} \\end{align*}\\] Terms \\(\\beta\\) is measure of power, between 0 and 1. \\(\\Phi\\) is the CDF of the normal distribution (think: area under the curve), and \\(\\Phi^{-1}\\) is its inverse. \\(\\mu_t - \\mu_c\\) is the difference in average outcomes in the treatment and control groups. \\(\\sigma\\) is the standard deviation of outcomes. \\(\\alpha\\) is our significance level - conventionally, 0.05. \\(N\\) is the total number of subjects. This is the only variable that is under the direct control of the researcher. Helpful video Recall that t-statistics beyond the critical values (e.g., 1.96) will result in rejecting the null hypothesis. We want to know the probability that our test statistic will fall in this rejection region. 6.3.2 Power in R To conduct a power analysis We need all but one of: sample size effect size in population standard deviation of outcome in population desired power level significance level What makes this calculation difficult? For continuous variables, we can calculate the power of either one-sample or two-sample test using the command power.t.test(n, delta, sd, sig.level, power, type, alternative). n is the number of observations; delta is the true difference in means; sd is the standard deviation within the population; sig.level is the test’s level of significance (Type I error probability); type is the type of t-test (“two.sample”, “one.sample” or “paired”); alternative specifies a direction of the test (“two.sided” or “one.sided”) power is the power of the test Note on effect sizes Cohen’s \\(d = \\frac{delta}{\\sigma}\\) = \\(\\frac{\\tt delta}{\\tt sd}\\) Problem: We usually don’t know \\(\\sigma\\) or delta. Solution 1: Use sample data for pooled standard deviation (\\(\\hat{s}_y\\)) and difference in means (\\(\\bar{y}_T - \\bar{y}_C\\)). Solution 2: Use rules of thumb, .2, .5, .8 (e.g., delta = .5 and d = 1}) Cohen,Jacob.1992.Statistical power analysis.Psychological Science ## Leave one argument blank or = NULL ## Power for an 800-person study with .25 effect size and 400-person groups power.t.test(n= 400, delta = .25, sd=1, sig.level = .05, power = NULL) ## ## Two-sample t test power calculation ## ## n = 400 ## delta = 0.25 ## sd = 1 ## sig.level = 0.05 ## power = 0.9419449 ## alternative = two.sided ## ## NOTE: n is number in *each* group ## What effect size would we need for 80% power? power.t.test(n= 400, delta = NULL, sd=1, sig.level = .05, power = .8) ## ## Two-sample t test power calculation ## ## n = 400 ## delta = 0.1983417 ## sd = 1 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided ## ## NOTE: n is number in *each* group 6.3.2.1 Additional Resources for Power in R Power analysis in Conjoint Experiments by Martin Lukac: tool R resource: Additional functions from Statmethods For proportions, The command power.prop.test(n, p1, p2, sig.level, power, alternative) may be used to calculate the power. Note that this command may only be used to calculate power for a two-sample test. n is the number of observations per group (assumes equal size); p1 the proportion in group 1; p2 the proportion in group 2; sig.level is the test’s level of significance alternative specifies a direction of the test (“two.sided” or “one.sided”); power specifies power of the test ## What sample size for difference in proportions at 80% power? power.prop.test(n=NULL, p1 = .75, p2=.80, sig.level=.10, power = .8) ## ## Two-sample comparison of proportions power calculation ## ## n = 861.4198 ## p1 = 0.75 ## p2 = 0.8 ## sig.level = 0.1 ## power = 0.8 ## alternative = two.sided ## ## NOTE: n is number in *each* group 6.3.3 Relationship between Error Rates and Multiple Testing \\[\\begin{align*} Pr(\\text{at least one significant result}) &amp;= 1 - Pr(\\text{no significant result})\\\\ &amp;= 1 - (1 - 0.05)^{\\text{number of tests}} \\end{align*}\\] With 20 tests, you have a 64% chance of observing at least one significant result even if all are not significant. 1 - (1 - 0.05)^20 ## [1] 0.6415141 For this reason, researchers may make adjustments to p-values when they have several tests in a single analysis. See EGAP’s resource "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
